{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c0403aa1010c4e99b80d615d037c4ade": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7297d884374e4d2fbf6bc84204254b53",
              "IPY_MODEL_8e26ed6d2c2d46ba8d6bbe528f87cbfd",
              "IPY_MODEL_e4d7704195474e08b8fa0f2ef755487a"
            ],
            "layout": "IPY_MODEL_8c79625577f74cb9b9a9b1daa4c32405"
          }
        },
        "7297d884374e4d2fbf6bc84204254b53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eaa6e47371f5475eb09af036389eff82",
            "placeholder": "​",
            "style": "IPY_MODEL_221effb821c14dc0a95ba688fdb39e3b",
            "value": "100%"
          }
        },
        "8e26ed6d2c2d46ba8d6bbe528f87cbfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_550588020623470b8448c70e54d95c5a",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b4941afcbe74f249250bbd40fb525b6",
            "value": 100
          }
        },
        "e4d7704195474e08b8fa0f2ef755487a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb7a83de815c492fa8c16fc6b2ffec58",
            "placeholder": "​",
            "style": "IPY_MODEL_ab0460c588c644fdb5ea84e9310622e4",
            "value": " 100/100 [00:01&lt;00:00, 77.62it/s]"
          }
        },
        "8c79625577f74cb9b9a9b1daa4c32405": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaa6e47371f5475eb09af036389eff82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "221effb821c14dc0a95ba688fdb39e3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "550588020623470b8448c70e54d95c5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b4941afcbe74f249250bbd40fb525b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb7a83de815c492fa8c16fc6b2ffec58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab0460c588c644fdb5ea84e9310622e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d1abb825f11481fb47f21f86532b87f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_66d79bdfb26a4a95b4048a001a0e5d8f",
              "IPY_MODEL_428aa3308ac64037ad2f394660be04e8",
              "IPY_MODEL_9d6586b305344c4f9cd0acaefde665b0"
            ],
            "layout": "IPY_MODEL_f259f394ad3d4a14b6efb6ebb8c04d0a"
          }
        },
        "66d79bdfb26a4a95b4048a001a0e5d8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6f971d456574c90afa178a15056f18a",
            "placeholder": "​",
            "style": "IPY_MODEL_6acf2a2415034af88137c0779bcd79cd",
            "value": ""
          }
        },
        "428aa3308ac64037ad2f394660be04e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb7197802ca444ed9205f3bf9d0d531f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dfa8fc377c5c4aa9ba2f4f0c8c34fe32",
            "value": 0
          }
        },
        "9d6586b305344c4f9cd0acaefde665b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33e7537ea8994ddc87a2ed5f8722dcce",
            "placeholder": "​",
            "style": "IPY_MODEL_41a51bbc335e4e9fb80b304d3d32e62e",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "f259f394ad3d4a14b6efb6ebb8c04d0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6f971d456574c90afa178a15056f18a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6acf2a2415034af88137c0779bcd79cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb7197802ca444ed9205f3bf9d0d531f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "dfa8fc377c5c4aa9ba2f4f0c8c34fe32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33e7537ea8994ddc87a2ed5f8722dcce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41a51bbc335e4e9fb80b304d3d32e62e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#📚 <font color=\"pink\"> ***Preparation*** 📚"
      ],
      "metadata": {
        "id": "QzWrCen14kz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## <font color=\"pink\"> **1. Mount Google Drive**\n",
        "#@markdown ---\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted at /content/drive\")\n",
        "\n",
        "# Define the base project directory on Google Drive\n",
        "# You can change 'TTS_Project' to whatever you prefer\n",
        "project_drive_path = '/content/drive/My Drive/Evatalk'\n",
        "\n",
        "# Create the base project directory if it doesn't exist\n",
        "if not os.path.exists(project_drive_path):\n",
        "    os.makedirs(project_drive_path)\n",
        "    print(f\"Created base project directory on Drive: {project_drive_path}\")\n",
        "else:\n",
        "    print(f\"Base project directory on Drive already exists: {project_drive_path}\")\n",
        "\n",
        "# Define paths for dataset and training outputs within the project directory\n",
        "dataset_drive_path = os.path.join(project_drive_path, 'Dataset')\n",
        "training_outputs_drive_path = os.path.join(project_drive_path, 'Output')\n",
        "checkpoints_drive_path = os.path.join(training_outputs_drive_path, 'checkpoints')\n",
        "logs_drive_path = os.path.join(training_outputs_drive_path, 'logs')\n",
        "\n",
        "# Create dataset and output directories on Drive if they don't exist\n",
        "for path in [dataset_drive_path, training_outputs_drive_path, checkpoints_drive_path, logs_drive_path]:\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "        print(f\"Created directory on Drive: {path}\")\n",
        "    else:\n",
        "        print(f\"Directory on Drive already exists: {path}\")\n",
        "\n",
        "# Set symbolic links or variables for easier access later in the script (optional but good practice)\n",
        "# For data loading and saving, we'll use the full paths directly in the subsequent cells"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQd4oC5ZyjFu",
        "outputId": "6e6209a7-d1ef-48b2-e77b-0083110b6c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted at /content/drive\n",
            "Base project directory on Drive already exists: /content/drive/My Drive/Evatalk\n",
            "Directory on Drive already exists: /content/drive/My Drive/Evatalk/Dataset\n",
            "Directory on Drive already exists: /content/drive/My Drive/Evatalk/Output\n",
            "Directory on Drive already exists: /content/drive/My Drive/Evatalk/Output/checkpoints\n",
            "Directory on Drive already exists: /content/drive/My Drive/Evatalk/Output/logs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # <font color=\"pink\"> **Check the GPU** 👁️ </font>\n",
        "#@markdown ---\n",
        "#@markdown #### It is not recommended to use the <font color=\"orange\">**K80**</font> card (although it works correctly, it is **time consuming**). Restart the runtime to **get another card**.\n",
        "\n",
        "!nvidia-smi -L"
      ],
      "metadata": {
        "id": "JYFlZBxo4q-F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e689bd1-55bf-417b-e97f-67cd97076f4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-37dbecb8-ed4b-e4dd-14df-20d475856471)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"pink\"> ***Training*** </font> 🤖"
      ],
      "metadata": {
        "id": "9FvaS6gv5_uN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## <font color=\"pink\"> **2. Install Tacotron2 (w/ARPAbet).** 📦\n",
        "%matplotlib inline\n",
        "import os\n",
        "import io\n",
        "\n",
        "# --- Ensure project_drive_path is defined if this cell is run independently ---\n",
        "# This variable should match the base project path defined in your \"1. Mount Google Drive\" cell\n",
        "project_drive_path = '/content/drive/My Drive/Evatalk'\n",
        "# --- End of project_drive_path definition ---\n",
        "\n",
        "%cd /content/\n",
        "if not os.path.isdir(\"/content/TTS-TT2/\"):\n",
        "  print(\"Cloning justinjohn0306/TTS-TT2\")\n",
        "  !git clone https://github.com/justinjohn0306/ARPAtaco2.git TTS-TT2\n",
        "  %cd /content/TTS-TT2/\n",
        "  !git submodule init\n",
        "  !git submodule update\n",
        "%cd /content/TTS-TT2/\n",
        "#NVIDIA's requirements\n",
        "#I believe Colab gives us PyTorch and TF by default so we don't need anything else\n",
        "#Versions specified in requirements.txt have conflicts so that's why we simply get current versions\n",
        "print(\"Downloading tacotron2 requirements\")\n",
        "!pip install matplotlib numpy inflect scipy Unidecode pillow\n",
        "#Our requirements\n",
        "#We'll need gdown to download some really cool things\n",
        "!pip install git+https://github.com/wkentaro/gdown.git\n",
        "import gdown\n",
        "!git submodule init\n",
        "!git submodule update\n",
        "!pip install ffmpeg-normalize\n",
        "!pip install -q unidecode tensorboardX\n",
        "!apt-get -qq install sox\n",
        "!apt-get install pv\n",
        "!apt-get install jq\n",
        "!wget https://raw.githubusercontent.com/tonikelope/megadown/master/megadown -O megadown.sh\n",
        "!chmod 755 megadown.sh\n",
        "#Download NVIDIA's LJSpeech model\n",
        "# Download NVIDIA's LJSpeech model using wget\n",
        "tt2_pretrained_url = \"https://github.com/justinjohn0306/ARPAtaco2/releases/download/pretrained_model/tacotron2_statedict.pt\"\n",
        "tt2_pretrained_path = \"/content/TTS-TT2/tacotron2_statedict.pt\"\n",
        "\n",
        "if not os.path.isfile(tt2_pretrained_path):\n",
        "  print(\"Downloading tt2 pretrained model using wget\")\n",
        "  !wget {tt2_pretrained_url} -O {tt2_pretrained_path}\n",
        "\n",
        "\n",
        "#tt2_pretrained = \"https://drive.google.com/uc?id=1c5ZTuT7J08wLUoVZ2KkUs_VdZuJ86ZqA\"\n",
        "#if not os.path.isfile(\"/content/TTS-TT2/pretrained_model\"):\n",
        "  #print(\"Downloading tt2 pretrained\")\n",
        "  #gdown.download(tt2_pretrained, \"/content/TTS-TT2/pretrained_model\", quiet=False)\n",
        "\n",
        "latest_downloaded = None\n",
        "\n",
        "import time\n",
        "import logging\n",
        "\n",
        "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
        "logging.getLogger('numba').setLevel(logging.WARNING)\n",
        "logging.getLogger('librosa').setLevel(logging.WARNING)\n",
        "\n",
        "\n",
        "import argparse\n",
        "import math\n",
        "from numpy import finfo\n",
        "\n",
        "import torch\n",
        "from distributed import apply_gradient_allreduce\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from model import Tacotron2\n",
        "from data_utils import TextMelLoader, TextMelCollate\n",
        "from loss_function import Tacotron2Loss\n",
        "from logger import Tacotron2Logger\n",
        "from hparams import create_hparams\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import layers\n",
        "from utils import load_wav_to_torch, load_filepaths_and_text\n",
        "from text import text_to_sequence\n",
        "from math import e\n",
        "#from tqdm import tqdm # Terminal\n",
        "#from tqdm import tqdm_notebook as tqdm # Legacy Notebook TQDM\n",
        "from tqdm.notebook import tqdm # Modern Notebook TQDM\n",
        "from distutils.dir_util import copy_tree\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "def download_from_google_drive(file_id, file_name):\n",
        "  # download a file from the Google Drive link\n",
        "  !rm -f ./cookie\n",
        "  !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id={file_id}\" > /dev/null\n",
        "  confirm_text = !awk '/download/ {print $NF}' ./cookie\n",
        "  confirm_text = confirm_text[0]\n",
        "  !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm={confirm_text}&id={file_id}\" -o {file_name}\n",
        "\n",
        "def create_mels():\n",
        "    print(\"Generating Mels\")\n",
        "    stft = layers.TacotronSTFT(\n",
        "                hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
        "                hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
        "                hparams.mel_fmax)\n",
        "    def save_mel(filename):\n",
        "        audio, sampling_rate = load_wav_to_torch(filename)\n",
        "        if sampling_rate != stft.sampling_rate:\n",
        "            raise ValueError(\"{} {} SR doesn't match target {} SR\".format(filename,\n",
        "                sampling_rate, stft.sampling_rate))\n",
        "        audio_norm = audio / hparams.max_wav_value\n",
        "        audio_norm = audio_norm.unsqueeze(0)\n",
        "        audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
        "        melspec = stft.mel_spectrogram(audio_norm)\n",
        "        melspec = torch.squeeze(melspec, 0).cpu().numpy()\n",
        "        np.save(filename.replace('.wav', ''), melspec)\n",
        "\n",
        "    import glob\n",
        "    wavs = glob.glob('wavs/*.wav')\n",
        "    for i in tqdm(wavs):\n",
        "        save_mel(i)\n",
        "\n",
        "\n",
        "def reduce_tensor(tensor, n_gpus):\n",
        "    rt = tensor.clone()\n",
        "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
        "    rt /= n_gpus\n",
        "    return rt\n",
        "\n",
        "\n",
        "def init_distributed(hparams, n_gpus, rank, group_name):\n",
        "    assert torch.cuda.is_available(), \"Distributed mode requires CUDA.\"\n",
        "    print(\"Initializing Distributed\")\n",
        "\n",
        "    # Set cuda device so everything is done on the right GPU.\n",
        "    torch.cuda.set_device(rank % torch.cuda.device_count())\n",
        "\n",
        "    # Initialize distributed communication\n",
        "    dist.init_process_group(\n",
        "        backend=hparams.dist_backend, init_method=hparams.dist_url,\n",
        "        world_size=n_gpus, rank=rank, group_name=group_name)\n",
        "\n",
        "    print(\"Done initializing distributed\")\n",
        "\n",
        "\n",
        "def prepare_dataloaders(hparams):\n",
        "    # Get data, data loaders and collate function ready\n",
        "    trainset = TextMelLoader(hparams.training_files, hparams)\n",
        "    valset = TextMelLoader(hparams.validation_files, hparams)\n",
        "    collate_fn = TextMelCollate(hparams.n_frames_per_step)\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        train_sampler = DistributedSampler(trainset)\n",
        "        shuffle = False\n",
        "    else:\n",
        "        train_sampler = None\n",
        "        shuffle = True\n",
        "\n",
        "    train_loader = DataLoader(trainset, num_workers=1, shuffle=shuffle,\n",
        "                              sampler=train_sampler,\n",
        "                              batch_size=hparams.batch_size, pin_memory=False,\n",
        "                              drop_last=True, collate_fn=collate_fn)\n",
        "    return train_loader, valset, collate_fn\n",
        "\n",
        "\n",
        "def prepare_directories_and_logger(output_directory, log_directory, rank):\n",
        "    if rank == 0:\n",
        "        if not os.path.isdir(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "            os.chmod(output_directory, 0o775)\n",
        "        logger = Tacotron2Logger(os.path.join(output_directory, log_directory))\n",
        "    else:\n",
        "        logger = None\n",
        "    return logger\n",
        "\n",
        "\n",
        "def load_model(hparams):\n",
        "    model = Tacotron2(hparams).cuda()\n",
        "    if hparams.fp16_run:\n",
        "        model.decoder.attention_layer.score_mask_value = finfo('float16').min\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        model = apply_gradient_allreduce(model)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def warm_start_model(checkpoint_path, model, ignore_layers):\n",
        "    assert os.path.isfile(checkpoint_path)\n",
        "    print(\"Warm starting model from checkpoint '{}'\".format(checkpoint_path))\n",
        "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model_dict = checkpoint_dict['state_dict']\n",
        "    if len(ignore_layers) > 0:\n",
        "        model_dict = {k: v for k, v in model_dict.items()\n",
        "                      if k not in ignore_layers}\n",
        "        dummy_dict = model.state_dict()\n",
        "        dummy_dict.update(model_dict)\n",
        "        model_dict = dummy_dict\n",
        "    model.load_state_dict(model_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer):\n",
        "    assert os.path.isfile(checkpoint_path)\n",
        "    print(\"Loading checkpoint '{}'\".format(checkpoint_path))\n",
        "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint_dict['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
        "    learning_rate = checkpoint_dict['learning_rate']\n",
        "    iteration = checkpoint_dict['iteration']\n",
        "    print(\"Loaded checkpoint '{}' from iteration {}\" .format(\n",
        "        checkpoint_path, iteration))\n",
        "    return model, optimizer, learning_rate, iteration\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, learning_rate, iteration, filepath):\n",
        "    import random\n",
        "    if True:\n",
        "        print(\"Saving model and optimizer state at iteration {} to {}\".format(\n",
        "            iteration, filepath))\n",
        "        try:\n",
        "            torch.save({'iteration': iteration,\n",
        "                    'state_dict': model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'learning_rate': learning_rate}, filepath)\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"interrupt received while saving, waiting for save to complete.\")\n",
        "            torch.save({'iteration': iteration,'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(),'learning_rate': learning_rate}, filepath)\n",
        "        print(\"Model Saved\")\n",
        "\n",
        "def plot_alignment(alignment, info=None):\n",
        "    %matplotlib inline\n",
        "    fig, ax = plt.subplots(figsize=(int(alignment_graph_width/100), int(alignment_graph_height/100)))\n",
        "    im = ax.imshow(alignment, cmap='inferno', aspect='auto', origin='lower',\n",
        "                   interpolation='none')\n",
        "    ax.autoscale(enable=True, axis=\"y\", tight=True)\n",
        "    fig.colorbar(im, ax=ax)\n",
        "    xlabel = 'Decoder timestep'\n",
        "    if info is not None:\n",
        "        xlabel += '\\n\\n' + info\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel('Encoder timestep')\n",
        "    plt.tight_layout()\n",
        "    fig.canvas.draw()\n",
        "    plt.show()\n",
        "\n",
        "def validate(model, criterion, valset, iteration, batch_size, n_gpus,\n",
        "             collate_fn, logger, distributed_run, rank, epoch, start_eposh, learning_rate):\n",
        "    \"\"\"Handles all the validation scoring and printing\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_sampler = DistributedSampler(valset) if distributed_run else None\n",
        "        val_loader = DataLoader(valset, sampler=val_sampler, num_workers=1,\n",
        "                                shuffle=False, batch_size=batch_size,\n",
        "                                pin_memory=False, collate_fn=collate_fn)\n",
        "\n",
        "        val_loss = 0.0\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            x, y = model.parse_batch(batch)\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            if distributed_run:\n",
        "                reduced_val_loss = reduce_tensor(loss.data, n_gpus).item()\n",
        "            else:\n",
        "                reduced_val_loss = loss.item()\n",
        "            val_loss += reduced_val_loss\n",
        "        val_loss = val_loss / (i + 1)\n",
        "\n",
        "    model.train()\n",
        "    if rank == 0:\n",
        "        print(\"Epoch: {} Validation loss {}: {:9f}  Time: {:.1f}m LR: {:.6f}\".format(epoch, iteration, val_loss,(time.perf_counter()-start_eposh)/60, learning_rate))\n",
        "        logger.log_validation(val_loss, model, y, y_pred, iteration)\n",
        "        if hparams.show_alignments:\n",
        "            %matplotlib inline\n",
        "            _, mel_outputs, gate_outputs, alignments = y_pred\n",
        "            idx = random.randint(0, alignments.size(0) - 1)\n",
        "            plot_alignment(alignments[idx].data.cpu().numpy().T)\n",
        "\n",
        "def train(output_directory, log_directory, checkpoint_path, warm_start, n_gpus,\n",
        "          rank, group_name, hparams, log_directory2, save_interval, backup_interval):\n",
        "    \"\"\"Training and validation logging results to tensorboard and stdout\n",
        "\n",
        "    Params\n",
        "    ------\n",
        "    output_directory (string): directory to save checkpoints\n",
        "    log_directory (string) directory to save tensorboard logs\n",
        "    checkpoint_path(string): checkpoint path\n",
        "    n_gpus (int): number of gpus\n",
        "    rank (int): rank of current gpu\n",
        "    hparams (object): comma separated list of \"name=value\" pairs.\n",
        "    \"\"\"\n",
        "    if hparams.distributed_run:\n",
        "        init_distributed(hparams, n_gpus, rank, group_name)\n",
        "\n",
        "    torch.manual_seed(hparams.seed)\n",
        "    torch.cuda.manual_seed(hparams.seed)\n",
        "\n",
        "    model = load_model(hparams)\n",
        "    learning_rate = hparams.learning_rate\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
        "                                 weight_decay=hparams.weight_decay)\n",
        "\n",
        "    if hparams.fp16_run:\n",
        "        from apex import amp\n",
        "        model, optimizer = amp.initialize(\n",
        "            model, optimizer, opt_level='O2')\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        model = apply_gradient_allreduce(model)\n",
        "\n",
        "    criterion = Tacotron2Loss()\n",
        "\n",
        "    logger = prepare_directories_and_logger(\n",
        "        output_directory, log_directory, rank)\n",
        "\n",
        "    train_loader, valset, collate_fn = prepare_dataloaders(hparams)\n",
        "\n",
        "    # Load checkpoint if one exists\n",
        "    iteration = 0\n",
        "    epoch_offset = 0\n",
        "    if checkpoint_path is not None and os.path.isfile(checkpoint_path):\n",
        "        if warm_start:\n",
        "            model = warm_start_model(\n",
        "                checkpoint_path, model, hparams.ignore_layers)\n",
        "        else:\n",
        "            model, optimizer, _learning_rate, iteration = load_checkpoint(\n",
        "                checkpoint_path, model, optimizer)\n",
        "            if hparams.use_saved_learning_rate:\n",
        "                learning_rate = _learning_rate\n",
        "            iteration += 1  # next iteration is iteration + 1\n",
        "            epoch_offset = max(0, int(iteration / len(train_loader)))\n",
        "    else:\n",
        "      os.path.isfile(\"/content/TTS-TT2/pretrained_model\")\n",
        "      %cd /dev/null\n",
        "      !/content/TTS-TT2/megadown.sh https://mega.nz/#!WXY3RILA!KyoGHtfB_sdhmLFoykG2lKWhh0GFdwMkk7OwAjpQHRo --o pretrained_model\n",
        "      %cd /content/TTS-TT2\n",
        "      model = warm_start_model(\"/content/TTS-TT2/pretrained_model\", model, hparams.ignore_layers)\n",
        "      # download LJSpeech pretrained model if no checkpoint already exists\n",
        "\n",
        "    start_eposh = time.perf_counter()\n",
        "    learning_rate = 0.0\n",
        "    model.train()\n",
        "    is_overflow = False\n",
        "    # ================ MAIN TRAINNIG LOOP! ===================\n",
        "    for epoch in tqdm(range(epoch_offset, hparams.epochs)):\n",
        "        print(\"\\nStarting Epoch: {} Iteration: {}\".format(epoch, iteration))\n",
        "        start_eposh = time.perf_counter() # eposh is russian, not a typo\n",
        "        for i, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "            start = time.perf_counter()\n",
        "            if iteration < hparams.decay_start: learning_rate = hparams.A_\n",
        "            else: iteration_adjusted = iteration - hparams.decay_start; learning_rate = (hparams.A_*(e**(-iteration_adjusted/hparams.B_))) + hparams.C_\n",
        "            learning_rate = max(hparams.min_learning_rate, learning_rate) # output the largest number\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = learning_rate\n",
        "\n",
        "            model.zero_grad()\n",
        "            x, y = model.parse_batch(batch)\n",
        "            y_pred = model(x)\n",
        "\n",
        "            loss = criterion(y_pred, y)\n",
        "            if hparams.distributed_run:\n",
        "                reduced_loss = reduce_tensor(loss.data, n_gpus).item()\n",
        "            else:\n",
        "                reduced_loss = loss.item()\n",
        "            if hparams.fp16_run:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            if hparams.fp16_run:\n",
        "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                    amp.master_params(optimizer), hparams.grad_clip_thresh)\n",
        "                is_overflow = math.isnan(grad_norm)\n",
        "            else:\n",
        "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                    model.parameters(), hparams.grad_clip_thresh)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            if not is_overflow and rank == 0:\n",
        "                duration = time.perf_counter() - start\n",
        "                logger.log_training(\n",
        "                    reduced_loss, grad_norm, learning_rate, duration, iteration)\n",
        "                #print(\"Batch {} loss {:.6f} Grad Norm {:.6f} Time {:.6f}\".format(iteration, reduced_loss, grad_norm, duration), end='\\r', flush=True)\n",
        "\n",
        "            iteration += 1\n",
        "        validate(model, criterion, valset, iteration,\n",
        "                 hparams.batch_size, n_gpus, collate_fn, logger,\n",
        "                 hparams.distributed_run, rank, epoch, start_eposh, learning_rate)\n",
        "        if (epoch+1) % save_interval == 0 or (epoch+1) == hparams.epochs: # not sure if the latter is necessary\n",
        "            save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path)\n",
        "        if backup_interval > 0 and (epoch+1) % backup_interval == 0:\n",
        "            save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path + \"_epoch_%s\" % (epoch+1))\n",
        "        if log_directory2 != None:\n",
        "            copy_tree(log_directory, log_directory2)\n",
        "def check_dataset(hparams):\n",
        "    from utils import load_wav_to_torch, load_filepaths_and_text\n",
        "    import os\n",
        "    import numpy as np\n",
        "    def check_arr(filelist_arr):\n",
        "        for i, file in enumerate(filelist_arr):\n",
        "            if len(file) > 2:\n",
        "                print(\"|\".join(file), \"\\nhas multiple '|', this may not be an error.\")\n",
        "            if hparams.load_mel_from_disk and '.wav' in file[0]:\n",
        "                print(\"[WARNING]\", file[0], \" in filelist while expecting .npy .\")\n",
        "            else:\n",
        "                if not hparams.load_mel_from_disk and '.npy' in file[0]:\n",
        "                    print(\"[WARNING]\", file[0], \" in filelist while expecting .wav .\")\n",
        "            if (not os.path.exists(file[0])):\n",
        "                print(\"|\".join(file), \"\\n[WARNING] does not exist.\")\n",
        "            if len(file[1]) < 3:\n",
        "                print(\"|\".join(file), \"\\n[info] has no/very little text.\")\n",
        "            if not ((file[1].strip())[-1] in r\"!?,.;:\"):\n",
        "                print(\"|\".join(file), \"\\n[info] has no ending punctuation.\")\n",
        "            mel_length = 1\n",
        "            if hparams.load_mel_from_disk and '.npy' in file[0]:\n",
        "                melspec = torch.from_numpy(np.load(file[0], allow_pickle=True))\n",
        "                mel_length = melspec.shape[1]\n",
        "            if mel_length == 0:\n",
        "                print(\"|\".join(file), \"\\n[WARNING] has 0 duration.\")\n",
        "    print(\"Checking Training Files\")\n",
        "    audiopaths_and_text = load_filepaths_and_text(hparams.training_files) # get split lines from training_files text file.\n",
        "    check_arr(audiopaths_and_text)\n",
        "    print(\"Checking Validation Files\")\n",
        "    audiopaths_and_text = load_filepaths_and_text(hparams.validation_files) # get split lines from validation_files text file.\n",
        "    check_arr(audiopaths_and_text)\n",
        "    print(\"Finished Checking\")\n",
        "\n",
        "warm_start=False#sorry bout that\n",
        "n_gpus=1\n",
        "rank=0\n",
        "group_name=None\n",
        "\n",
        "# ---- DEFAULT PARAMETERS DEFINED HERE ----\n",
        "hparams = create_hparams()\n",
        "model_filename = 'current_model'\n",
        "hparams.training_files = \"filelists/clipper_train_filelist.txt\"\n",
        "hparams.validation_files = \"filelists/clipper_val_filelist.txt\"\n",
        "#hparams.use_mmi=True,          # not used in this notebook\n",
        "#hparams.use_gaf=True,          # not used in this notebook\n",
        "#hparams.max_gaf=0.5,           # not used in this notebook\n",
        "#hparams.drop_frame_rate = 0.2  # not used in this notebook\n",
        "hparams.p_attention_dropout=0.1\n",
        "hparams.p_decoder_dropout=0.1\n",
        "hparams.decay_start = 15000\n",
        "hparams.A_ = 5e-4\n",
        "hparams.B_ = 8000\n",
        "hparams.C_ = 0\n",
        "hparams.min_learning_rate = 1e-5\n",
        "generate_mels = True\n",
        "hparams.show_alignments = True\n",
        "alignment_graph_height = 600\n",
        "alignment_graph_width = 1000\n",
        "hparams.batch_size = 32\n",
        "hparams.load_mel_from_disk = True\n",
        "hparams.ignore_layers = []\n",
        "hparams.epochs = 10000\n",
        "torch.backends.cudnn.enabled = hparams.cudnn_enabled\n",
        "torch.backends.cudnn.benchmark = hparams.cudnn_benchmark\n",
        "output_directory = '/content/drive/My Drive/colab/outdir' # Location to save Checkpoints\n",
        "log_directory = '/content/TTS-TT2/logs' # Location to save Log files locally\n",
        "log_directory2 = '/content/drive/My Drive/colab/logs' # Location to copy log files (done at the end of each epoch to cut down on I/O)\n",
        "checkpoint_path = output_directory+(r'/')+model_filename\n",
        "\n",
        "# ---- Replace .wav with .npy in filelists ----\n",
        "!sed -i -- 's,.wav|,.npy|,g' filelists/*.txt\n",
        "!sed -i -- 's,.wav|,.npy|,g' {hparams.training_files}\n",
        "!sed -i -- 's,.wav|,.npy|,g' {hparams.validation_files}\n",
        "# ---- Replace .wav with .npy in filelists ----\n",
        "\n",
        "%cd /content/TTS-TT2\n",
        "\n",
        "data_path = 'wavs'\n",
        "!mkdir {data_path}"
      ],
      "metadata": {
        "id": "YoL_G-LV6rh5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e99dabba-190e-47a3-d037-2cf14fb07860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning justinjohn0306/TTS-TT2\n",
            "Cloning into 'TTS-TT2'...\n",
            "remote: Enumerating objects: 89, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 89 (delta 2), reused 1 (delta 0), pack-reused 80 (from 1)\u001b[K\n",
            "Receiving objects: 100% (89/89), 7.11 MiB | 9.08 MiB/s, done.\n",
            "Resolving deltas: 100% (30/30), done.\n",
            "/content/TTS-TT2\n",
            "/content/TTS-TT2\n",
            "Downloading tacotron2 requirements\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.11/dist-packages (7.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.16.1)\n",
            "Collecting Unidecode\n",
            "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: more_itertools>=8.5.0 in /usr/local/lib/python3.11/dist-packages (from inflect) (10.7.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from inflect) (4.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: typing_extensions>=4.14.0 in /usr/local/lib/python3.11/dist-packages (from typeguard>=4.0.1->inflect) (4.14.1)\n",
            "Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.4.0\n",
            "Collecting git+https://github.com/wkentaro/gdown.git\n",
            "  Cloning https://github.com/wkentaro/gdown.git to /tmp/pip-req-build-vogzscpk\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/wkentaro/gdown.git /tmp/pip-req-build-vogzscpk\n",
            "  Resolved https://github.com/wkentaro/gdown.git to commit c7c1b9d2bae2be11112bbe88de1f64ac90416cf2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown==5.2.1.dev6+gc7c1b9d) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown==5.2.1.dev6+gc7c1b9d) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown==5.2.1.dev6+gc7c1b9d) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown==5.2.1.dev6+gc7c1b9d) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown==5.2.1.dev6+gc7c1b9d) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown==5.2.1.dev6+gc7c1b9d) (4.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==5.2.1.dev6+gc7c1b9d) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==5.2.1.dev6+gc7c1b9d) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==5.2.1.dev6+gc7c1b9d) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==5.2.1.dev6+gc7c1b9d) (2025.8.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==5.2.1.dev6+gc7c1b9d) (1.7.1)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-5.2.1.dev6+gc7c1b9d-py3-none-any.whl size=18401 sha256=87027908fea3546eba68012594c2b9f78c8405940ebc1db023517ed6a2a3053e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6vqh2h8y/wheels/e0/20/b3/6a8a4105e26f8a19628074efb5bd5f3543a370060e53b7a748\n",
            "Successfully built gdown\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 5.2.0\n",
            "    Uninstalling gdown-5.2.0:\n",
            "      Successfully uninstalled gdown-5.2.0\n",
            "Successfully installed gdown-5.2.1.dev6+gc7c1b9d\n",
            "Collecting ffmpeg-normalize\n",
            "  Downloading ffmpeg_normalize-1.32.5-py3-none-any.whl.metadata (33 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from ffmpeg-normalize) (4.67.1)\n",
            "Collecting ffmpeg-progress-yield (from ffmpeg-normalize)\n",
            "  Downloading ffmpeg_progress_yield-1.0.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting colorlog (from ffmpeg-normalize)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting mutagen (from ffmpeg-normalize)\n",
            "  Downloading mutagen-1.47.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading ffmpeg_normalize-1.32.5-py3-none-any.whl (36 kB)\n",
            "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading ffmpeg_progress_yield-1.0.1-py3-none-any.whl (13 kB)\n",
            "Downloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mutagen, ffmpeg-progress-yield, colorlog, ffmpeg-normalize\n",
            "Successfully installed colorlog-6.9.0 ffmpeg-normalize-1.32.5 ffmpeg-progress-yield-1.0.1 mutagen-1.47.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hSelecting previously unselected package libopencore-amrnb0:amd64.\n",
            "(Reading database ... 126380 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libopencore-amrnb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
            "Preparing to unpack .../1-libopencore-amrwb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libsox3:amd64.\n",
            "Preparing to unpack .../2-libsox3_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-alsa:amd64.\n",
            "Preparing to unpack .../3-libsox-fmt-alsa_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libwavpack1:amd64.\n",
            "Preparing to unpack .../4-libwavpack1_5.4.0-1build2_amd64.deb ...\n",
            "Unpacking libwavpack1:amd64 (5.4.0-1build2) ...\n",
            "Selecting previously unselected package libsox-fmt-base:amd64.\n",
            "Preparing to unpack .../5-libsox-fmt-base_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package sox.\n",
            "Preparing to unpack .../6-sox_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking sox (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Setting up libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libwavpack1:amd64 (5.4.0-1build2) ...\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Setting up libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up sox (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  doc-base\n",
            "The following NEW packages will be installed:\n",
            "  pv\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 44.5 kB of archives.\n",
            "After this operation, 139 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 pv amd64 1.6.6-1build2 [44.5 kB]\n",
            "Fetched 44.5 kB in 1s (62.0 kB/s)\n",
            "Selecting previously unselected package pv.\n",
            "(Reading database ... 126453 files and directories currently installed.)\n",
            "Preparing to unpack .../pv_1.6.6-1build2_amd64.deb ...\n",
            "Unpacking pv (1.6.6-1build2) ...\n",
            "Setting up pv (1.6.6-1build2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "jq is already the newest version (1.6-2.1ubuntu3.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "--2025-08-15 08:22:00--  https://raw.githubusercontent.com/tonikelope/megadown/master/megadown\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17043 (17K) [text/plain]\n",
            "Saving to: ‘megadown.sh’\n",
            "\n",
            "megadown.sh         100%[===================>]  16.64K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-08-15 08:22:00 (27.9 MB/s) - ‘megadown.sh’ saved [17043/17043]\n",
            "\n",
            "Downloading tt2 pretrained model using wget\n",
            "--2025-08-15 08:22:00--  https://github.com/justinjohn0306/ARPAtaco2/releases/download/pretrained_model/tacotron2_statedict.pt\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/659617282/e7cd481b-4369-4d97-a5cd-8d541cddc7f8?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-08-15T09%3A03%3A35Z&rscd=attachment%3B+filename%3Dtacotron2_statedict.pt&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-08-15T08%3A02%3A41Z&ske=2025-08-15T09%3A03%3A35Z&sks=b&skv=2018-11-09&sig=e7SlAyMzumI8jsR%2Bjf5wUMR%2FcOJD1aBXmlKUxvmPgYQ%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1NTI0NjQyMCwibmJmIjoxNzU1MjQ2MTIwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.UPUwcp2ZOkt752gu52gZz75ZwrlLYQAptkqXbH5Oc-w&response-content-disposition=attachment%3B%20filename%3Dtacotron2_statedict.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-08-15 08:22:00--  https://release-assets.githubusercontent.com/github-production-release-asset/659617282/e7cd481b-4369-4d97-a5cd-8d541cddc7f8?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-08-15T09%3A03%3A35Z&rscd=attachment%3B+filename%3Dtacotron2_statedict.pt&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-08-15T08%3A02%3A41Z&ske=2025-08-15T09%3A03%3A35Z&sks=b&skv=2018-11-09&sig=e7SlAyMzumI8jsR%2Bjf5wUMR%2FcOJD1aBXmlKUxvmPgYQ%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1NTI0NjQyMCwibmJmIjoxNzU1MjQ2MTIwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.UPUwcp2ZOkt752gu52gZz75ZwrlLYQAptkqXbH5Oc-w&response-content-disposition=attachment%3B%20filename%3Dtacotron2_statedict.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 112818746 (108M) [application/octet-stream]\n",
            "Saving to: ‘/content/TTS-TT2/tacotron2_statedict.pt’\n",
            "\n",
            "/content/TTS-TT2/ta 100%[===================>] 107.59M  39.2MB/s    in 2.7s    \n",
            "\n",
            "2025-08-15 08:22:03 (39.2 MB/s) - ‘/content/TTS-TT2/tacotron2_statedict.pt’ saved [112818746/112818746]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/TTS-TT2/text/__init__.py:84: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  return s in _symbol_to_id and s is not '_' and s is not '~'\n",
            "/content/TTS-TT2/text/__init__.py:84: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  return s in _symbol_to_id and s is not '_' and s is not '~'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sed: can't read filelists/clipper_train_filelist.txt: No such file or directory\n",
            "sed: can't read filelists/clipper_val_filelist.txt: No such file or directory\n",
            "/content/TTS-TT2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## <font color=\"pink\"> **3. Upload/Load your Audio Dataset.** 🔊\n",
        "#@markdown Remember that they must be in a compatible format, i.e., sample rate 22050, 16 bit, mono.\n",
        "#@markdown If you do not have the audios in this format, check this box to make the conversion, apart from normalization and silence removal.\n",
        "audio_processing = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown #### Option 1: Path to your dataset on Google Drive (e.g., \"/content/drive/My Drive/Evatalk/Dataset\")\n",
        "#@markdown #### (Leave empty to upload from your local machine)\n",
        "dataset_drive_path = \"/content/drive/My Drive/Evatalk/trainrec.zip\" #@param {type:\"string\"}\n",
        "\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "import wave\n",
        "import shutil\n",
        "import datetime\n",
        "\n",
        "# --- Ensure project_drive_path is defined if this cell is run independently ---\n",
        "# This variable should match the base project path defined in your \"1. Mount Google Drive\" cell\n",
        "# If running sequentially, it should already be defined. Re-defining here for robustness.\n",
        "project_drive_path = '/content/drive/My Drive/Evatalk'\n",
        "# --- End of project_drive_path definition ---\n",
        "\n",
        "\n",
        "# Clear previous WAV files in the Colab environment if any\n",
        "if os.path.exists('/content/TTS-TT2/wavs/'): # Check if dir exists before listing\n",
        "  if os.listdir('/content/TTS-TT2/wavs/'):\n",
        "    print(\"Clearing existing audio files in /content/TTS-TT2/wavs/\")\n",
        "    !rm /content/TTS-TT2/wavs/*\n",
        "else:\n",
        "  # If the directory doesn't exist, create it (it should be created by git clone, but being safe)\n",
        "  os.makedirs('/content/TTS-TT2/wavs', exist_ok=True)\n",
        "\n",
        "\n",
        "# Create a shell script for audio processing\n",
        "# This remains the same as it performs local processing after files are in /content/TTS-TT2/wavs/\n",
        "with open('/content/audios.sh', 'w') as rsh:\n",
        "    rsh.write('''\\\n",
        "for file in /content/TTS-TT2/wavs/*.wav\n",
        "do\n",
        "    ffmpeg -y -i \"$file\" -ar 22050 /content/tempwav/srtmp.wav -loglevel error\n",
        "    ffmpeg -y -i /content/tempwav/srtmp.wav -c copy -fflags +bitexact -flags:v +bitexact -flags:a +bitexact -ar 22050 /content/tempwav/poop.wav -loglevel error\n",
        "    rm \"$file\"\n",
        "    mv /content/tempwav/poop.wav \"$file\"\n",
        "    rm /content/tempwav/*\n",
        "done\n",
        "''')\n",
        "\n",
        "# --- Logic to load from Drive or local upload ---\n",
        "dataset_drive_path = dataset_drive_path.strip()\n",
        "destination_colab_path = '/content/TTS-TT2/wavs/'\n",
        "\n",
        "if dataset_drive_path:\n",
        "  # Option 1: Load from Google Drive\n",
        "  print(f\"\\n\\033[34m\\033[1mAttempting to load dataset from Google Drive: {dataset_drive_path}\")\n",
        "  if os.path.exists(dataset_drive_path):\n",
        "    if os.path.isdir(dataset_drive_path):\n",
        "      # If it's a directory, copy its contents (assuming 'wavs' subfolder or direct wavs)\n",
        "      print(f\"Copying files from {dataset_drive_path} to {destination_colab_path}...\")\n",
        "      # Use rsync for efficiency if many files, otherwise shutil.copytree is fine\n",
        "      # If your Drive folder directly contains WAVs, use: !cp -r \"$dataset_drive_path\"/*.wav \"$destination_colab_path\"\n",
        "      # If your Drive folder contains a 'wavs' subfolder:\n",
        "      if os.path.exists(os.path.join(dataset_drive_path, 'wavs')):\n",
        "          !cp -r \"$dataset_drive_path\"/wavs/*.wav \"$destination_colab_path\"\n",
        "          print(\"Copied WAV files from 'wavs' subfolder on Drive.\")\n",
        "      else:\n",
        "          # Assume WAVs are directly in the specified dataset_drive_path\n",
        "          !cp -r \"$dataset_drive_path\"/*.wav \"$destination_colab_path\"\n",
        "          print(\"Copied WAV files directly from specified Drive folder.\")\n",
        "\n",
        "    elif zipfile.is_zipfile(dataset_drive_path):\n",
        "      # If it's a zip file, unzip it\n",
        "      print(f\"Unzipping {dataset_drive_path} from Drive to {destination_colab_path}...\")\n",
        "      !unzip -q -j \"$dataset_drive_path\" -d \"$destination_colab_path\"\n",
        "    else:\n",
        "      print(f\"\\n\\033[31m\\033[1m[ERROR] '{dataset_drive_path}' is not a valid directory or zip file. \"\n",
        "            \"Please check the path or use local upload.\")\n",
        "      # Fallback to local upload if Drive path is invalid\n",
        "      print(f\"\\n\\033[34m\\033[1mUpload your dataset(audios) from your local machine instead...\")\n",
        "      uploaded = files.upload()\n",
        "      for fn in uploaded.keys():\n",
        "        if zipfile.is_zipfile(fn):\n",
        "          print(f\"Unzipping {fn}...\")\n",
        "          !unzip -q -j \"$fn\" -d \"$destination_colab_path\"\n",
        "          !rm \"$fn\"\n",
        "        else:\n",
        "          print(f\"Moving {fn} to {destination_colab_path}...\")\n",
        "          shutil.move(fn, destination_colab_path + fn)\n",
        "  else:\n",
        "    print(f\"\\n\\033[33m\\033[1m[NOTICE] Google Drive path '{dataset_drive_path}' not found. \"\n",
        "          \"Falling back to local upload.\")\n",
        "    print(f\"\\n\\033[34m\\033[1mUpload your dataset(audios) from your local machine...\")\n",
        "    uploaded = files.upload()\n",
        "    for fn in uploaded.keys():\n",
        "      if zipfile.is_zipfile(fn):\n",
        "        print(f\"Unzipping {fn}...\")\n",
        "        !unzip -q -j \"$fn\" -d \"$destination_colab_path\"\n",
        "        !rm \"$fn\"\n",
        "      else:\n",
        "        print(f\"Moving {fn} to {destination_colab_path}...\")\n",
        "        shutil.move(fn, destination_colab_path + fn)\n",
        "else:\n",
        "  # Option 2: Local Upload (original behavior)\n",
        "  print(f\"\\n\\033[34m\\033[1mUpload your dataset(audios) from your local machine...\")\n",
        "  uploaded = files.upload()\n",
        "\n",
        "  for fn in uploaded.keys():\n",
        "    if zipfile.is_zipfile(fn):\n",
        "      print(f\"Unzipping {fn}...\")\n",
        "      !unzip -q -j \"$fn\" -d \"$destination_colab_path\" # Use destination_colab_path\n",
        "      !rm \"$fn\"\n",
        "    else:\n",
        "      print(f\"Moving {fn} to {destination_colab_path}...\") # Use destination_colab_path\n",
        "      shutil.move(fn, destination_colab_path + fn)\n",
        "\n",
        "# --- Common post-upload/copy cleanup and processing ---\n",
        "# Check if a 'wavs' subdirectory was created inside the target, which happens if zip/copy included it\n",
        "if os.path.exists(os.path.join(destination_colab_path, 'wavs')):\n",
        "    print(\"Detected 'wavs' subfolder, moving contents up...\")\n",
        "    for file in os.listdir(os.path.join(destination_colab_path, 'wavs')):\n",
        "      shutil.move(os.path.join(destination_colab_path, 'wavs', file), destination_colab_path)\n",
        "    !rmdir {os.path.join(destination_colab_path, 'wavs')} # Remove empty subdirectory after moving files\n",
        "\n",
        "# Clean up any residual list.txt file (shouldn't be in wavs dir anyway, but good to check)\n",
        "if os.path.exists(os.path.join(destination_colab_path, \"list.txt\")):\n",
        "    print(\"Warning: Found list.txt in audio directory, removing it.\")\n",
        "    !rm {os.path.join(destination_colab_path, \"list.txt\")}\n",
        "\n",
        "# Process audios if enabled\n",
        "if audio_processing:\n",
        "  print(f\"\\n\\033[37mMetadata removal and audio verification (sample rate, bit depth, mono conversion, normalization, silence removal)...\")\n",
        "  !mkdir -p /content/tempwav\n",
        "  !bash /content/audios.sh\n",
        "  !rm -r /content/tempwav # Clean up tempwav directory after processing\n",
        "\n",
        "totalduration = 0\n",
        "# Ensure we are in the correct directory to list files for duration check\n",
        "%cd /content/TTS-TT2/wavs\n",
        "for file_name in [x for x in os.listdir() if os.path.isfile(x) and x.endswith(\".wav\")]:\n",
        "    try:\n",
        "        with wave.open(file_name, \"rb\") as wave_file:\n",
        "            frames = wave_file.getnframes()\n",
        "            rate = wave_file.getframerate()\n",
        "            duration = frames / float(rate)\n",
        "            totalduration += duration\n",
        "\n",
        "            if duration >= 12:\n",
        "              print(f\"\\n\\033[33m\\033[1m[NOTICE] {file_name} is longer than 12 seconds. Lack of RAM can\"\n",
        "                    \" occur in a large batch size!\")\n",
        "    except wave.Error as e:\n",
        "        print(f\"\\n\\033[31m\\033[1m[ERROR] Could not process {file_name}: {e}\")\n",
        "        # Optionally, delete the problematic file: !rm \"$file_name\"\n",
        "\n",
        "wav_count = len([f for f in os.listdir(\"/content/TTS-TT2/wavs\") if f.endswith(\".wav\")])\n",
        "print(f\"\\n{wav_count} processed audios. total duration: {str(datetime.timedelta(seconds=round(totalduration, 0)))}\\n\")\n",
        "\n",
        "print(\"\\n\\033[32m\\033[1mAll set, please proceed.\")"
      ],
      "metadata": {
        "id": "0nfZSegP8Phu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d805135e-48cd-4b00-8c33-5033fd711c28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mAttempting to load dataset from Google Drive: /content/drive/My Drive/Evatalk/trainrec.zip\n",
            "Unzipping /content/drive/My Drive/Evatalk/trainrec.zip from Drive to /content/TTS-TT2/wavs/...\n",
            "\n",
            "\u001b[37mMetadata removal and audio verification (sample rate, bit depth, mono conversion, normalization, silence removal)...\n",
            "/content/TTS-TT2/wavs\n",
            "\n",
            "100 processed audios. total duration: 0:06:02\n",
            "\n",
            "\n",
            "\u001b[32m\u001b[1mAll set, please proceed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## <font color=\"pink\"> **4. Upload the transcript.** 📝\n",
        "#@markdown The transcript must be a ``.TXT`` file formatted in <font color=\"red\" size=\"+3\"> ``UTF-8 without BOM.``\n",
        "\n",
        "#@markdown #### Enter the path to your transcript file on Google Drive (e.g., \"/content/drive/My Drive/Evatalk/Dataset/list.txt\")\n",
        "#@markdown #### Leave empty to upload from your local machine.\n",
        "transcript_path = \"\" #@param {type: \"string\"}\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "import shutil # Import shutil for moving files\n",
        "\n",
        "# --- Ensure project_drive_path is defined if this cell is run independently ---\n",
        "# This variable should match the base project path defined in your \"1. Mount Google Drive\" cell\n",
        "# If running sequentially, it should already be defined. Re-defining here for robustness.\n",
        "project_drive_path = '/content/drive/My Drive/Evatalk'\n",
        "# --- End of project_drive_path definition ---\n",
        "\n",
        "%cd /content/TTS-TT2/filelists/\n",
        "\n",
        "# Clean up any existing list.txt\n",
        "if os.path.exists('/content/TTS-TT2/filelists/list.txt'):\n",
        "  !rm /content/TTS-TT2/filelists/list.txt\n",
        "\n",
        "transcript_path = transcript_path.strip()\n",
        "\n",
        "if transcript_path:\n",
        "  # Path provided, attempt to copy the file\n",
        "  if os.path.exists(transcript_path):\n",
        "    print(f\"\\n\\033[34m\\033[1mTranscript imported from: {transcript_path}\\n\\033[90m\")\n",
        "    try:\n",
        "      shutil.copy(transcript_path, '/content/TTS-TT2/filelists/list.txt')\n",
        "      listfn = 'list.txt' # Set listfn for subsequent processing\n",
        "    except Exception as e:\n",
        "      print(f\"\\n\\033[31m\\033[1m[ERROR] Could not copy file from {transcript_path}: {e}\")\n",
        "      print(f\"\\n\\033[34m\\033[1mPlease upload your transcript(list) manually...\")\n",
        "      uploaded = files.upload()\n",
        "      listfn, length = uploaded.popitem()\n",
        "      if listfn != \"list.txt\":\n",
        "        !mv \"$listfn\" list.txt\n",
        "  else:\n",
        "    print(f\"\\n\\033[33m\\033[1m[NOTICE] The path '{transcript_path}' is not found, check for errors and try again.\")\n",
        "    print(f\"\\n\\033[34m\\033[1mUpload your transcript(list)...\")\n",
        "    uploaded = files.upload()\n",
        "    listfn, length = uploaded.popitem()\n",
        "    if listfn != \"list.txt\":\n",
        "      !mv \"$listfn\" list.txt\n",
        "else:\n",
        "  # No path provided, prompt for local upload\n",
        "  print(\"\\n\\033[34m\\033[1mUpload your transcript(list)...\")\n",
        "  uploaded = files.upload()\n",
        "  listfn, length = uploaded.popitem()\n",
        "\n",
        "  if listfn != \"list.txt\":\n",
        "    !mv \"$listfn\" list.txt\n",
        "\n",
        "# --- Validate and filter transcript entries ---\n",
        "print(\"\\n\\033[37mValidating transcript entries...\")\n",
        "try:\n",
        "    with open('list.txt', 'r', encoding='utf-8') as f: # Specify encoding for safety\n",
        "        lines = f.readlines()\n",
        "except FileNotFoundError:\n",
        "    print(\"\\n\\033[31m\\033[1m[ERROR] list.txt was not found after upload/copy. Please re-run and ensure a file is provided.\")\n",
        "    # Exit or handle gracefully if transcript is missing\n",
        "    raise SystemExit(\"Transcript file missing.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n\\033[31m\\033[1m[ERROR] Could not read list.txt: {e}. Please ensure it's a valid UTF-8 TXT file.\")\n",
        "    raise SystemExit(\"Error reading transcript file.\")\n",
        "\n",
        "\n",
        "new_lines = []\n",
        "missing_audios = 0\n",
        "total_lines = len(lines)\n",
        "\n",
        "for i, line in enumerate(lines):\n",
        "    parts = line.strip().split('|')\n",
        "    if len(parts) >= 1: # Ensure there's at least an audio file path\n",
        "        # Assuming the audio file path is the first part, and it's relative to /content/TTS-TT2/\n",
        "        # The original code implies paths like 'wavs/audio.wav'\n",
        "        audio_file_path_relative = parts[0].strip()\n",
        "        audio_file_full_path = os.path.join('/content/TTS-TT2/wavs/', audio_file_path_relative)\n",
        "\n",
        "        if os.path.exists(audio_file_full_path):\n",
        "            new_lines.append(line)\n",
        "        else:\n",
        "            missing_audios += 1\n",
        "            print(f\"\\033[90mSkipping line {i+1}: Audio file not found for '{audio_file_path_relative}'\")\n",
        "    else:\n",
        "        print(f\"\\033[90mSkipping line {i+1}: Invalid format (expected 'audio_path|text'). Line: '{line.strip()}'\")\n",
        "\n",
        "\n",
        "if missing_audios > 0:\n",
        "    print(f\"\\n\\033[33m\\033[1m[NOTICE] {missing_audios} out of {total_lines} transcript entries were skipped because their corresponding audio files were not found in /content/TTS-TT2/ (e.g., /content/TTS-TT2/wavs/).\")\n",
        "    print(\"Please ensure your transcript paths match the actual location of your audio files.\")\n",
        "    if len(new_lines) == 0:\n",
        "        print(\"\\n\\033[31m\\033[1m[CRITICAL] No valid transcript entries remain after filtering. Training will likely fail.\")\n",
        "\n",
        "with open('list.txt', 'w', encoding='utf-8') as f: # Write with UTF-8 encoding\n",
        "    f.writelines(new_lines)\n",
        "\n",
        "%cd /content/TTS-TT2/\n",
        "print(\"\\n\\033[32m\\033[1mAll set, please proceed.\")"
      ],
      "metadata": {
        "id": "wOJ1Vl1E9l1K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "38f80468-8b12-401a-9ac2-f61882cdee3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TTS-TT2/filelists\n",
            "\n",
            "\u001b[34m\u001b[1mUpload your transcript(list)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e908b34f-494f-4c48-aeac-9ac393a3bd05\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e908b34f-494f-4c48-aeac-9ac393a3bd05\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving metadata.csv to metadata.csv\n",
            "\n",
            "\u001b[37mValidating transcript entries...\n",
            "\u001b[90mSkipping line 1: Audio file not found for '0001'\n",
            "\u001b[90mSkipping line 2: Audio file not found for '0002'\n",
            "\u001b[90mSkipping line 3: Audio file not found for '0003'\n",
            "\u001b[90mSkipping line 4: Audio file not found for '0004'\n",
            "\u001b[90mSkipping line 5: Audio file not found for '0005'\n",
            "\u001b[90mSkipping line 6: Audio file not found for '0006'\n",
            "\u001b[90mSkipping line 7: Audio file not found for '0007'\n",
            "\u001b[90mSkipping line 8: Audio file not found for '0008'\n",
            "\u001b[90mSkipping line 9: Audio file not found for '0009'\n",
            "\u001b[90mSkipping line 10: Audio file not found for '0010'\n",
            "\u001b[90mSkipping line 11: Audio file not found for '0011'\n",
            "\u001b[90mSkipping line 12: Audio file not found for '0012'\n",
            "\u001b[90mSkipping line 13: Audio file not found for '0013'\n",
            "\u001b[90mSkipping line 14: Audio file not found for '0014'\n",
            "\u001b[90mSkipping line 15: Audio file not found for '0015'\n",
            "\u001b[90mSkipping line 16: Audio file not found for '0016'\n",
            "\u001b[90mSkipping line 17: Audio file not found for '0017'\n",
            "\u001b[90mSkipping line 18: Audio file not found for '0018'\n",
            "\u001b[90mSkipping line 19: Audio file not found for '0019'\n",
            "\u001b[90mSkipping line 20: Audio file not found for '0020'\n",
            "\u001b[90mSkipping line 21: Audio file not found for '0021'\n",
            "\u001b[90mSkipping line 22: Audio file not found for '0022'\n",
            "\u001b[90mSkipping line 23: Audio file not found for '0023'\n",
            "\u001b[90mSkipping line 24: Audio file not found for '0024'\n",
            "\u001b[90mSkipping line 25: Audio file not found for '0025'\n",
            "\u001b[90mSkipping line 26: Audio file not found for '0026'\n",
            "\u001b[90mSkipping line 27: Audio file not found for '0027'\n",
            "\u001b[90mSkipping line 28: Audio file not found for '0028'\n",
            "\u001b[90mSkipping line 29: Audio file not found for '0029'\n",
            "\u001b[90mSkipping line 30: Audio file not found for '0030'\n",
            "\u001b[90mSkipping line 31: Audio file not found for '0031'\n",
            "\u001b[90mSkipping line 32: Audio file not found for '0032'\n",
            "\u001b[90mSkipping line 33: Audio file not found for '0033'\n",
            "\u001b[90mSkipping line 34: Audio file not found for '0034'\n",
            "\u001b[90mSkipping line 35: Audio file not found for '0035'\n",
            "\u001b[90mSkipping line 36: Audio file not found for '0036'\n",
            "\u001b[90mSkipping line 37: Audio file not found for '0037'\n",
            "\u001b[90mSkipping line 38: Audio file not found for '0038'\n",
            "\u001b[90mSkipping line 39: Audio file not found for '0039'\n",
            "\u001b[90mSkipping line 40: Audio file not found for '0040'\n",
            "\u001b[90mSkipping line 41: Audio file not found for '0041'\n",
            "\u001b[90mSkipping line 42: Audio file not found for '0042'\n",
            "\u001b[90mSkipping line 43: Audio file not found for '0043'\n",
            "\u001b[90mSkipping line 44: Audio file not found for '0044'\n",
            "\u001b[90mSkipping line 45: Audio file not found for '0045'\n",
            "\u001b[90mSkipping line 46: Audio file not found for '0046'\n",
            "\u001b[90mSkipping line 47: Audio file not found for '0047'\n",
            "\u001b[90mSkipping line 48: Audio file not found for '0048'\n",
            "\u001b[90mSkipping line 49: Audio file not found for '0049'\n",
            "\u001b[90mSkipping line 50: Audio file not found for '0050'\n",
            "\u001b[90mSkipping line 51: Audio file not found for '0051'\n",
            "\u001b[90mSkipping line 52: Audio file not found for '0052'\n",
            "\u001b[90mSkipping line 53: Audio file not found for '0053'\n",
            "\u001b[90mSkipping line 54: Audio file not found for '0054'\n",
            "\u001b[90mSkipping line 55: Audio file not found for '0055'\n",
            "\u001b[90mSkipping line 56: Audio file not found for '0056'\n",
            "\u001b[90mSkipping line 57: Audio file not found for '0057'\n",
            "\u001b[90mSkipping line 58: Audio file not found for '0058'\n",
            "\u001b[90mSkipping line 59: Audio file not found for '0059'\n",
            "\u001b[90mSkipping line 60: Audio file not found for '0060'\n",
            "\u001b[90mSkipping line 61: Audio file not found for '0061'\n",
            "\u001b[90mSkipping line 62: Audio file not found for '0062'\n",
            "\u001b[90mSkipping line 63: Audio file not found for '0063'\n",
            "\u001b[90mSkipping line 64: Audio file not found for '0064'\n",
            "\u001b[90mSkipping line 65: Audio file not found for '0065'\n",
            "\u001b[90mSkipping line 66: Audio file not found for '0066'\n",
            "\u001b[90mSkipping line 67: Audio file not found for '0067'\n",
            "\u001b[90mSkipping line 68: Audio file not found for '0068'\n",
            "\u001b[90mSkipping line 69: Audio file not found for '0069'\n",
            "\u001b[90mSkipping line 70: Audio file not found for '0070'\n",
            "\u001b[90mSkipping line 71: Audio file not found for '0071'\n",
            "\u001b[90mSkipping line 72: Audio file not found for '0072'\n",
            "\u001b[90mSkipping line 73: Audio file not found for '0073'\n",
            "\u001b[90mSkipping line 74: Audio file not found for '0074'\n",
            "\u001b[90mSkipping line 75: Audio file not found for '0075'\n",
            "\u001b[90mSkipping line 76: Audio file not found for '0076'\n",
            "\u001b[90mSkipping line 77: Audio file not found for '0077'\n",
            "\u001b[90mSkipping line 78: Audio file not found for '0078'\n",
            "\u001b[90mSkipping line 79: Audio file not found for '0079'\n",
            "\u001b[90mSkipping line 80: Audio file not found for '0080'\n",
            "\u001b[90mSkipping line 81: Audio file not found for '0081'\n",
            "\u001b[90mSkipping line 82: Audio file not found for '0082'\n",
            "\u001b[90mSkipping line 83: Audio file not found for '0083'\n",
            "\u001b[90mSkipping line 84: Audio file not found for '0084'\n",
            "\u001b[90mSkipping line 85: Audio file not found for '0085'\n",
            "\u001b[90mSkipping line 86: Audio file not found for '0086'\n",
            "\u001b[90mSkipping line 87: Audio file not found for '0087'\n",
            "\u001b[90mSkipping line 88: Audio file not found for '0088'\n",
            "\u001b[90mSkipping line 89: Audio file not found for '0089'\n",
            "\u001b[90mSkipping line 90: Audio file not found for '0090'\n",
            "\u001b[90mSkipping line 91: Audio file not found for '0091'\n",
            "\u001b[90mSkipping line 92: Audio file not found for '0092'\n",
            "\u001b[90mSkipping line 93: Audio file not found for '0093'\n",
            "\u001b[90mSkipping line 94: Audio file not found for '0094'\n",
            "\u001b[90mSkipping line 95: Audio file not found for '0095'\n",
            "\u001b[90mSkipping line 96: Audio file not found for '0096'\n",
            "\u001b[90mSkipping line 97: Audio file not found for '0097'\n",
            "\u001b[90mSkipping line 98: Audio file not found for '0098'\n",
            "\u001b[90mSkipping line 99: Audio file not found for '0099'\n",
            "\u001b[90mSkipping line 100: Audio file not found for '0100'\n",
            "\n",
            "\u001b[33m\u001b[1m[NOTICE] 100 out of 100 transcript entries were skipped because their corresponding audio files were not found in /content/TTS-TT2/ (e.g., /content/TTS-TT2/wavs/).\n",
            "Please ensure your transcript paths match the actual location of your audio files.\n",
            "\n",
            "\u001b[31m\u001b[1m[CRITICAL] No valid transcript entries remain after filtering. Training will likely fail.\n",
            "/content/TTS-TT2\n",
            "\n",
            "\u001b[32m\u001b[1mAll set, please proceed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## <font color=\"pink\"> **4. Upload the transcript.** 📝\n",
        "#@markdown The transcript must be a ``.TXT`` file formatted in <font color=\"red\" size=\"+3\"> ``UTF-8 without BOM.``\n",
        "\n",
        "#@markdown #### Enter the path to your transcript file on Google Drive (e.g., \"/content/drive/My Drive/Evatalk/Dataset/list.txt\")\n",
        "#@markdown #### Leave empty to upload from your local machine.\n",
        "transcript_path = \"/content/drive/My Drive/Evatalk/Dataset/list.txt\" #@param {type: \"string\"} <-- **SET THIS PATH!**\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# --- Ensure project_drive_path is defined if this cell is run independently ---\n",
        "project_drive_path = '/content/drive/My Drive/Evatalk'\n",
        "# --- End of project_drive_path definition ---\n",
        "\n",
        "%cd /content/TTS-TT2/filelists/\n",
        "\n",
        "if os.path.exists('/content/TTS-TT2/filelists/list.txt'):\n",
        "  !rm /content/TTS-TT2/filelists/list.txt\n",
        "\n",
        "transcript_path = transcript_path.strip()\n",
        "\n",
        "if transcript_path:\n",
        "  if os.path.exists(transcript_path):\n",
        "    print(f\"\\n\\033[34m\\033[1mTranscript imported from: {transcript_path}\\n\\033[90m\")\n",
        "    try:\n",
        "      shutil.copy(transcript_path, '/content/TTS-TT2/filelists/list.txt')\n",
        "      listfn = 'list.txt'\n",
        "    except Exception as e:\n",
        "      print(f\"\\n\\033[31m\\033[1m[ERROR] Could not copy file from {transcript_path}: {e}\")\n",
        "      print(f\"\\n\\033[34m\\033[1mPlease upload your transcript(list) manually...\")\n",
        "      uploaded = files.upload()\n",
        "      listfn, length = uploaded.popitem()\n",
        "      if listfn != \"list.txt\":\n",
        "        !mv \"$listfn\" list.txt\n",
        "  else:\n",
        "    print(f\"\\n\\033[33m\\033[1m[NOTICE] The path '{transcript_path}' is not found, check for errors and try again.\")\n",
        "    print(f\"\\n\\033[34m\\033[1mUpload your transcript(list)...\")\n",
        "    uploaded = files.upload()\n",
        "    listfn, length = uploaded.popitem()\n",
        "    if listfn != \"list.txt\":\n",
        "      !mv \"$listfn\" list.txt\n",
        "else:\n",
        "  print(\"\\n\\033[34m\\033[1mUpload your transcript(list)...\")\n",
        "  uploaded = files.upload()\n",
        "  listfn, length = uploaded.popitem()\n",
        "\n",
        "  if listfn != \"list.txt\":\n",
        "    !mv \"$listfn\" list.txt\n",
        "\n",
        "print(\"\\n\\033[37mValidating transcript entries...\")\n",
        "try:\n",
        "    with open('list.txt', 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "except FileNotFoundError:\n",
        "    print(\"\\n\\033[31m\\033[1m[ERROR] list.txt was not found after upload/copy. Please re-run and ensure a file is provided.\")\n",
        "    raise SystemExit(\"Transcript file missing.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n\\033[31m\\033[1m[ERROR] Could not read list.txt: {e}. Please ensure it's a valid UTF-8 TXT file.\")\n",
        "    raise SystemExit(\"Error reading transcript file.\")\n",
        "\n",
        "new_lines = []\n",
        "missing_audios = 0\n",
        "total_lines = len(lines)\n",
        "\n",
        "for i, line in enumerate(lines):\n",
        "    parts = line.strip().split('|')\n",
        "    if len(parts) >= 1:\n",
        "        audio_file_path_relative = parts[0].strip()\n",
        "        audio_file_full_path = os.path.join('/content/TTS-TT2/wavs/', audio_file_path_relative)\n",
        "\n",
        "        if os.path.exists(audio_file_full_path):\n",
        "            new_lines.append(line)\n",
        "        else:\n",
        "            missing_audios += 1\n",
        "            print(f\"\\033[90mSkipping line {i+1}: Audio file not found for '{audio_file_full_path}'\")\n",
        "    else:\n",
        "        print(f\"\\033[90mSkipping line {i+1}: Invalid format (expected 'audio_path|text'). Line: '{line.strip()}'\")\n",
        "\n",
        "if missing_audios > 0:\n",
        "    print(f\"\\n\\033[33m\\033[1m[NOTICE] {missing_audios} out of {total_lines} transcript entries were skipped because their corresponding audio files were not found in /content/TTS-TT2/ (e.g., /content/TTS-TT2/wavs/).\")\n",
        "    print(\"Please ensure your transcript paths match the actual location of your audio files.\")\n",
        "    if len(new_lines) == 0:\n",
        "        print(\"\\n\\033[31m\\033[1m[CRITICAL] No valid transcript entries remain after filtering. Training will likely fail.\")\n",
        "\n",
        "with open('list.txt', 'w', encoding='utf-8') as f:\n",
        "    f.writelines(new_lines)\n",
        "\n",
        "%cd /content/TTS-TT2/\n",
        "print(\"\\n\\033[32m\\033[1mAll set, please proceed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "6P-Lbh1_8QyQ",
        "outputId": "9fe40adc-5467-4792-a4cc-662831c2925a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TTS-TT2/filelists\n",
            "\n",
            "\u001b[33m\u001b[1m[NOTICE] The path '/content/drive/My Drive/Evatalk/Dataset/list.txt' is not found, check for errors and try again.\n",
            "\n",
            "\u001b[34m\u001b[1mUpload your transcript(list)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-583816fe-f564-49f0-8b21-72f45cc9fb73\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-583816fe-f564-49f0-8b21-72f45cc9fb73\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving lists.txt to lists.txt\n",
            "\n",
            "\u001b[37mValidating transcript entries...\n",
            "/content/TTS-TT2\n",
            "\n",
            "\u001b[32m\u001b[1mAll set, please proceed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## <font color=\"pink\"> **5. Configure the model parameters.** 🎛️\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #### Your desired model name:\n",
        "\n",
        "model_filename = 'Alex' #@param {type: \"string\"}\n",
        "\n",
        "#@markdown #### Upload your transcription / text to TTS-TT2/filelists and right click -> copy path:\n",
        "Training_file = \"filelists/list.txt\" #@param {type: \"string\"}\n",
        "hparams.training_files = Training_file\n",
        "hparams.validation_files = Training_file\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# hparams to Tune\n",
        "#hparams.use_mmi=True,          # not used in this notebook\n",
        "#hparams.use_gaf=True,          # not used in this notebook\n",
        "#hparams.max_gaf=0.5,           # not used in this notebook\n",
        "#hparams.drop_frame_rate = 0.2  # not used in this notebook\n",
        "hparams.p_attention_dropout=0.1\n",
        "hparams.p_decoder_dropout=0.1\n",
        "\n",
        "# Learning Rate             # https://www.desmos.com/calculator/ptgcz4vzsw / http://boards.4channel.org/mlp/thread/34778298#p34789030\n",
        "hparams.decay_start = 15000         # wait till decay_start to start decaying learning rate\n",
        "\n",
        "#@markdown #### Lower learning rates will take more time but will lead to more accurate results:\n",
        "# Start/Max Learning Rate\n",
        "hparams.A_ = 3e-4 #@param [\"3e-6\", \"1e-5\", \"1e-4\", \"5e-4\", \"1e-3\"] {type:\"raw\", allow-input: true}\n",
        "hparams.B_ = 8000                   # Decay Rate\n",
        "hparams.C_ = 0                      # Shift learning rate equation by this value\n",
        "hparams.min_learning_rate = 1e-5    # Min Learning Rate\n",
        "\n",
        "# Quality of Life\n",
        "generate_mels = True\n",
        "hparams.show_alignments = True\n",
        "alignment_graph_height = 600\n",
        "alignment_graph_width = 1000\n",
        "\n",
        "#@markdown #### Your batch size, lower if you don't have enough ram:\n",
        "\n",
        "hparams.batch_size =  8#@param {type: \"integer\"}\n",
        "hparams.load_mel_from_disk = True\n",
        "hparams.ignore_layers = [] # Layers to reset (None by default, other than foreign languages this param can be ignored)\n",
        "use_cmudict = True #@param {type:\"boolean\"}\n",
        "#@markdown #### Your total epochs to train to. Not recommended to change:\n",
        "\n",
        "##@markdown #### Amount of epochs before stopping, preferably a very high amount to not stop.\n",
        "hparams.epochs =  250#@param {type: \"integer\"}\n",
        "\n",
        "torch.backends.cudnn.enabled = hparams.cudnn_enabled\n",
        "torch.backends.cudnn.benchmark = hparams.cudnn_benchmark\n",
        "\n",
        "#@markdown #### Where to save your model when training:\n",
        "output_directory = '/content/drive/MyDrive/colab/outdir' #@param {type: \"string\"}\n",
        "log_directory = '/content/TTS-TT2/logs' # Location to save Log files locally\n",
        "log_directory2 = '/content/drive/My Drive/colab/logs' # Location to copy log files (done at the end of each epoch to cut down on I/O)\n",
        "checkpoint_path = output_directory+(r'/')+model_filename\n",
        "\n",
        "##@markdown #### Train the model from scratch? (If yes, then uncheck the box below):\n",
        "#warm_start=True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "hparams.text_cleaners=[\"english_cleaners\"] + ([\"cmudict_cleaners\"] if use_cmudict is True else [])\n",
        "\n",
        "\n",
        "#@markdown Note:-\n",
        "\n",
        "#@markdown - The learning_rate value is ordered from smallest to largest, top to bottom.\n",
        "\n",
        "#@markdown - The smaller the \"learning rates\" value is, the longer it will take to train the model, but the more accurate the results will be.\n",
        "\n",
        "#@markdown ___\n",
        "\n",
        "#@markdown Todo:-\n",
        "#@markdown - Disable warm_start\n",
        "#@markdown - Add tensorboard training monitor\n",
        "\n",
        "#@markdown ___\n"
      ],
      "metadata": {
        "id": "wRpZet5m-BCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## <font color=\"pink\"> **5. Adjust Transcript Paths** ⚙️\n",
        "#@markdown This cell ensures that the audio file paths in your `list.txt` correctly point to the `wavs/` directory inside your Colab environment.\n",
        "\n",
        "import os\n",
        "\n",
        "print(\"Adjusting paths in list.txt...\")\n",
        "\n",
        "list_file_path = '/content/TTS-TT2/filelists/list.txt'\n",
        "\n",
        "try:\n",
        "    with open(list_file_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    updated_lines = []\n",
        "    changes_made = 0\n",
        "\n",
        "    # Define the desired prefix for paths in list.txt\n",
        "    # This should match where your WAV/NPY files are stored in Colab\n",
        "    correct_prefix_for_data = \"wavs/\"\n",
        "\n",
        "    for line in lines:\n",
        "        parts = line.strip().split('|')\n",
        "        if len(parts) >= 1:\n",
        "            original_data_path_in_transcript = parts[0].strip() # e.g., '0001.wav', 'trainrec/0001.wav', 'wavs/0001.wav'\n",
        "\n",
        "            # Get just the filename (e.g., '0001.wav' from any of the above)\n",
        "            filename_only = os.path.basename(original_data_path_in_transcript)\n",
        "\n",
        "            # Construct the desired path: 'wavs/filename.wav'\n",
        "            # .replace('\\\\', '/') ensures forward slashes even on Windows systems\n",
        "            desired_data_path_in_transcript = os.path.join(correct_prefix_for_data, filename_only).replace('\\\\', '/')\n",
        "\n",
        "            if original_data_path_in_transcript != desired_data_path_in_transcript:\n",
        "                updated_line = desired_data_path_in_transcript + '|' + '|'.join(parts[1:])\n",
        "                updated_lines.append(updated_line + '\\n')\n",
        "                changes_made += 1\n",
        "            else:\n",
        "                updated_lines.append(line) # No change needed, keep original line\n",
        "        else:\n",
        "            updated_lines.append(line) # Keep invalid lines, they'll be skipped by validation later\n",
        "\n",
        "\n",
        "    if changes_made > 0:\n",
        "        with open(list_file_path, 'w', encoding='utf-8') as f:\n",
        "            f.writelines(updated_lines)\n",
        "        print(f\"\\n\\033[32m\\033[1mSuccessfully adjusted {changes_made} paths in list.txt to use '{correct_prefix_for_data}' prefix.\")\n",
        "        print(f\"Example transformation: '{original_data_path_in_transcript}' -> '{desired_data_path_in_transcript}'\")\n",
        "    else:\n",
        "        print(f\"\\n\\033[33m\\033[1mNo path adjustments needed or made. Paths already use '{correct_prefix_for_data}' prefix.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\n\\033[31m\\033[1m[ERROR] list.txt not found at {list_file_path}. Please ensure the previous cell (transcript upload) ran correctly.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n\\033[31m\\033[1m[ERROR] An error occurred while adjusting transcript paths: {e}\")\n",
        "\n",
        "# Re-validation part: Check if WAV files (the source data) exist based on the adjusted paths.\n",
        "print(\"\\nRe-validating transcript entries against source WAV files...\")\n",
        "try:\n",
        "    with open(list_file_path, 'r', encoding='utf-8') as f:\n",
        "        lines_recheck = f.readlines()\n",
        "\n",
        "    valid_lines_count = 0\n",
        "    total_lines_recheck = len(lines_recheck)\n",
        "\n",
        "    for line in lines_recheck:\n",
        "        parts = line.strip().split('|')\n",
        "        if len(parts) >= 1:\n",
        "            # The path in list.txt *should* now be 'wavs/0001.wav' (or similar)\n",
        "            data_path_in_list = parts[0].strip()\n",
        "\n",
        "            # Construct the full path to the *WAV* file to check its existence\n",
        "            # This is important before Mels are generated; we're checking if source WAVs exist\n",
        "            # Replace .npy back to .wav for this check, in case the list.txt already had .npy (unlikely if order is followed)\n",
        "            full_wav_file_path_to_check = os.path.join('/content/TTS-TT2/', data_path_in_list.replace('.npy', '.wav'))\n",
        "\n",
        "            if os.path.exists(full_wav_file_path_to_check):\n",
        "                valid_lines_count += 1\n",
        "            else:\n",
        "                print(f\"\\033[90mWarning: Source WAV not found for '{data_path_in_list}' (expected at {full_wav_file_path_to_check})\")\n",
        "\n",
        "    if valid_lines_count == total_lines_recheck and valid_lines_count > 0:\n",
        "        print(f\"\\n\\033[32m\\033[1mAll {valid_lines_count} transcript entries now point to existing WAV audio files! You can proceed.\")\n",
        "    elif valid_lines_count > 0:\n",
        "        print(f\"\\n\\033[33m\\033[1m[NOTICE] {valid_lines_count} out of {total_lines_recheck} transcript entries point to existing WAV files after adjustment. Still some missing or malformed entries.\")\n",
        "        print(\"Please review your audio files and transcript if you expect more valid entries.\")\n",
        "    else:\n",
        "        print(\"\\n\\033[31m\\033[1m[CRITICAL] Still no valid transcript entries found pointing to WAV files after adjustment.\")\n",
        "        print(\"This means your audio files are NOT in /content/TTS-TT2/wavs/ OR your list.txt is completely malformed/empty.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n\\033[31m\\033[1m[ERROR] Error during final re-validation: {e}\")\n",
        "\n",
        "%cd /content/TTS-TT2/ # Ensure we are in the base directory for the next steps"
      ],
      "metadata": {
        "id": "ou6UnbAE-a40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "478336c1-ef5e-4c74-ffc8-27d9e733f83e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting paths in list.txt...\n",
            "\n",
            "\u001b[32m\u001b[1mSuccessfully adjusted 100 paths in list.txt to use 'wavs/' prefix.\n",
            "Example transformation: '0100.wav' -> 'wavs/0100.wav'\n",
            "\n",
            "Re-validating transcript entries against source WAV files...\n",
            "\n",
            "\u001b[32m\u001b[1mAll 100 transcript entries now point to existing WAV audio files! You can proceed.\n",
            "[Errno 2] No such file or directory: '/content/TTS-TT2/ # Ensure we are in the base directory for the next steps'\n",
            "/content/TTS-TT2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## <font color=\"pink\"> **6. Convert the .WAV files to Mel spectrograms and check the files.** 🔉>📈\n",
        "\n",
        "# --- Start of necessary imports and helper functions ---\n",
        "# Ensure these are defined only once in your notebook or in appropriate places.\n",
        "# I'm including them here for a self-contained, runnable cell.\n",
        "\n",
        "import time\n",
        "import logging\n",
        "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
        "logging.getLogger('numba').setLevel(logging.WARNING)\n",
        "logging.getLogger('librosa').setLevel(logging.WARNING)\n",
        "\n",
        "import argparse\n",
        "import math\n",
        "from numpy import finfo\n",
        "\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import layers\n",
        "from utils import load_wav_to_torch, load_filepaths_and_text\n",
        "from text import text_to_sequence\n",
        "from math import e\n",
        "from tqdm.notebook import tqdm # Modern Notebook TQDM\n",
        "from distutils.dir_util import copy_tree\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "from hparams import create_hparams # Needed for stft settings\n",
        "\n",
        "# Define create_mels function\n",
        "def create_mels():\n",
        "    print(\"Generating Mels\")\n",
        "    # Make sure hparams is created before calling this\n",
        "    _hparams = create_hparams() # Re-create hparams locally if it might not be globally available\n",
        "    stft = layers.TacotronSTFT(\n",
        "                _hparams.filter_length, _hparams.hop_length, _hparams.win_length,\n",
        "                _hparams.n_mel_channels, _hparams.sampling_rate, _hparams.mel_fmin,\n",
        "                _hparams.mel_fmax)\n",
        "\n",
        "    def save_mel(filename):\n",
        "        # filename will be relative, e.g., 'wavs/0001.wav' from glob\n",
        "        audio, sampling_rate = load_wav_to_torch(filename)\n",
        "        if sampling_rate != stft.sampling_rate:\n",
        "            raise ValueError(\"{} {} SR doesn't match target {} SR\".format(filename,\n",
        "                sampling_rate, stft.sampling_rate))\n",
        "        audio_norm = audio / _hparams.max_wav_value\n",
        "        audio_norm = audio_norm.unsqueeze(0)\n",
        "        audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
        "        melspec = stft.mel_spectrogram(audio_norm)\n",
        "        melspec = torch.squeeze(melspec, 0).cpu().numpy()\n",
        "        # np.save expects path relative to current dir or absolute path\n",
        "        # Since we're in /content/TTS-TT2/, 'wavs/0001' becomes /content/TTS-TT2/wavs/0001.npy\n",
        "        np.save(filename.replace('.wav', ''), melspec)\n",
        "\n",
        "    import glob\n",
        "    # Ensure current directory is /content/TTS-TT2/ when calling glob\n",
        "    current_dir_before_mels = os.getcwd()\n",
        "    if current_dir_before_mels != '/content/TTS-TT2':\n",
        "        %cd /content/TTS-TT2\n",
        "        print(f\"Changed directory to {os.getcwd()} for mel generation.\")\n",
        "\n",
        "    wavs = glob.glob('wavs/*.wav')\n",
        "    if not wavs:\n",
        "        print(\"\\n\\033[31m\\033[1m[ERROR] No WAV files found in /content/TTS-TT2/wavs/. Please check your audio upload step.\")\n",
        "        return # Exit if no wavs found\n",
        "\n",
        "    for i in tqdm(wavs):\n",
        "        save_mel(i)\n",
        "\n",
        "    if current_dir_before_mels != '/content/TTS-TT2': # Change back if we changed it\n",
        "        %cd {current_dir_before_mels}\n",
        "        print(f\"Changed directory back to {os.getcwd()}\")\n",
        "\n",
        "\n",
        "# Define check_dataset function (with the crucial path fix)\n",
        "def check_dataset(hparams):\n",
        "    from utils import load_filepaths_and_text\n",
        "    import os\n",
        "    import numpy as np\n",
        "\n",
        "    def check_arr(filelist_arr):\n",
        "        for i, file in enumerate(filelist_arr):\n",
        "            # file[0] will be like 'wavs/0001.npy' (after sed command)\n",
        "\n",
        "            # Construct the full absolute path for checking existence and loading\n",
        "            # This is the crucial fix: always use the absolute path from /content/TTS-TT2/\n",
        "            full_data_path_in_colab = os.path.join('/content/TTS-TT2/', file[0].strip())\n",
        "\n",
        "            if len(file) > 2:\n",
        "                print(\"|\".join(file), \"\\nhas multiple '|', this may not be an error.\")\n",
        "            if hparams.load_mel_from_disk and '.wav' in file[0]:\n",
        "                print(\"[WARNING]\", file[0], \" in filelist while expecting .npy .\")\n",
        "            else:\n",
        "                if not hparams.load_mel_from_disk and '.npy' in file[0]:\n",
        "                    print(\"[WARNING]\", file[0], \" in filelist while expecting .wav .\")\n",
        "\n",
        "            # Check existence using the full absolute path\n",
        "            if (not os.path.exists(full_data_path_in_colab)):\n",
        "                print(f\"{'|'.join(file)}\\n[WARNING] does not exist (expected at: {full_data_path_in_colab}).\")\n",
        "            if len(file[1]) < 3:\n",
        "                print(\"|\".join(file), \"\\n[info] has no/very little text.\")\n",
        "            if not ((file[1].strip())[-1] in r\"!?,.;:\"):\n",
        "                print(\"|\".join(file), \"\\n[info] has no ending punctuation.\")\n",
        "            mel_length = 1\n",
        "            if hparams.load_mel_from_disk and '.npy' in file[0]:\n",
        "                # Load the mel spectrogram using the full absolute path\n",
        "                try:\n",
        "                    melspec = torch.from_numpy(np.load(full_data_path_in_colab, allow_pickle=True))\n",
        "                    mel_length = melspec.shape[1]\n",
        "                except Exception as e:\n",
        "                    print(f\"\\n\\033[31m\\033[1m[ERROR] Could not load mel spectrogram from '{full_data_path_in_colab}': {e}\")\n",
        "                    mel_length = 0 # Indicate failure to load\n",
        "            if mel_length == 0:\n",
        "                print(\"|\".join(file), \"\\n[WARNING] has 0 duration (or failed to load mel).\")\n",
        "\n",
        "    print(\"Checking Training Files\")\n",
        "    # Ensure current directory is /content/TTS-TT2/ when loading filepaths and text\n",
        "    current_dir_before_check = os.getcwd()\n",
        "    if current_dir_before_check != '/content/TTS-TT2':\n",
        "        %cd /content/TTS-TT2\n",
        "        print(f\"Changed directory to {os.getcwd()} for dataset check.\")\n",
        "\n",
        "    audiopaths_and_text = load_filepaths_and_text(hparams.training_files)\n",
        "    check_arr(audiopaths_and_text)\n",
        "    print(\"Checking Validation Files\")\n",
        "    audiopaths_and_text = load_filepaths_and_text(hparams.validation_files)\n",
        "    check_arr(audiopaths_and_text)\n",
        "    print(\"Finished Checking\")\n",
        "\n",
        "    if current_dir_before_check != '/content/TTS-TT2': # Change back if we changed it\n",
        "        %cd {current_dir_before_check}\n",
        "        print(f\"Changed directory back to {os.getcwd()}\")\n",
        "\n",
        "\n",
        "# --- End of functions and imports ---\n",
        "\n",
        "# Ensure hparams is defined before use (from previous cell or re-define here for testing)\n",
        "try:\n",
        "    _ = hparams\n",
        "except NameError:\n",
        "    hparams = create_hparams() # Create if not already created\n",
        "    # Ensure relevant hparams are set if testing this cell independently\n",
        "    hparams.training_files = \"filelists/clipper_train_filelist.txt\"\n",
        "    hparams.validation_files = \"filelists/clipper_val_filelist.txt\"\n",
        "    hparams.load_mel_from_disk = True # Crucial for this step\n",
        "\n",
        "# This variable should be controlled by a checkbox in your Colab UI,\n",
        "# but for running this code block, ensure it's True if you want mels generated.\n",
        "generate_mels = True # @param {type:\"boolean\"} # Adjust if this param comes from a Colab UI checkbox\n",
        "\n",
        "\n",
        "if generate_mels:\n",
        "    create_mels()\n",
        "\n",
        "print(\"Checking for missing files\")\n",
        "# ---- Replace .wav with .npy in filelists ----\n",
        "# Make sure we are in /content/TTS-TT2/ for these sed commands to apply to the correct filelists\n",
        "%cd /content/TTS-TT2/\n",
        "\n",
        "# These commands update the filelists to point to .npy files instead of .wav files\n",
        "!sed -i -- 's,.wav|,.npy|,g' {hparams.training_files}\n",
        "!sed -i -- 's,.wav|,.npy|,g' {hparams.validation_files}\n",
        "\n",
        "check_dataset(hparams)\n",
        "\n",
        "# Change back to the project root for consistency if needed for subsequent cells\n",
        "%cd /content/TTS-TT2/\n",
        "print(\"\\n\\033[32m\\033[1mMel spectrograms generated and files checked. Proceed to training!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "c0403aa1010c4e99b80d615d037c4ade",
            "7297d884374e4d2fbf6bc84204254b53",
            "8e26ed6d2c2d46ba8d6bbe528f87cbfd",
            "e4d7704195474e08b8fa0f2ef755487a",
            "8c79625577f74cb9b9a9b1daa4c32405",
            "eaa6e47371f5475eb09af036389eff82",
            "221effb821c14dc0a95ba688fdb39e3b",
            "550588020623470b8448c70e54d95c5a",
            "8b4941afcbe74f249250bbd40fb525b6",
            "fb7a83de815c492fa8c16fc6b2ffec58",
            "ab0460c588c644fdb5ea84e9310622e4"
          ]
        },
        "id": "FH-RsJV93WEB",
        "outputId": "6ae6b975-b7a7-4b7a-910b-2d3838f730a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating Mels\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0403aa1010c4e99b80d615d037c4ade"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for missing files\n",
            "/content/TTS-TT2\n",
            "Checking Training Files\n",
            "Checking Validation Files\n",
            "Finished Checking\n",
            "/content/TTS-TT2\n",
            "\n",
            "\u001b[32m\u001b[1mMel spectrograms generated and files checked. Proceed to training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## <b><font color=\"pink\"> **7. Check the working cmudict patch** 🩹\n",
        "%cd /content/TTS-TT2/\n",
        "import text\n",
        "print(text.sequence_to_text(text.text_to_sequence(\"We must capture an Earth creature, K 9, and return it back with us to Mars.\", [\"cmudict_cleaners\", \"english_cleaners\"])))"
      ],
      "metadata": {
        "id": "-ae0Z3n-__NX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ef07cba-4d34-4874-b089-b0a0163d516e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TTS-TT2\n",
            "{W IY1} {M AH1 S T} {K AE1 P CH ER0} {AE1 N} {ER1 TH} {K R IY1 CH ER0} , {K EY1} nine , {AH0 N D} {R IH0 T ER1 N} {IH1 T} {B AE1 K} {W IH1 DH} {AH1 S} {T UW1} {M AA1 R Z} .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # <font color=\"pink\"> **8. Begin training.** 🏋️\n",
        "#@markdown ___\n",
        "#@markdown ### How often to save (number of epochs)\n",
        "#@markdown `10` by default. Raise this if you're hitting a rate limit. If you're using a particularly large dataset, you might want to set this to `1` to prevent loss of progress.\n",
        "save_interval =  10#@param {type: \"integer\"}\n",
        "#\n",
        "#@markdown ### How often to backup (number of epochs)\n",
        "#@markdown `-1` (disabled) by default. This will save extra copies of your model every so often, so you always have something to revert to if you train the model for too long. This *will* chew through your Google Drive storage.\n",
        "backup_interval =  -1#@param {type: \"integer\"}\n",
        "#\n",
        "\n",
        "print('FP16 Run:', hparams.fp16_run)\n",
        "print('Dynamic Loss Scaling:', hparams.dynamic_loss_scaling)\n",
        "print('Distributed Run:', hparams.distributed_run)\n",
        "print('cuDNN Enabled:', hparams.cudnn_enabled)\n",
        "print('cuDNN Benchmark:', hparams.cudnn_benchmark)\n",
        "train(output_directory, log_directory, checkpoint_path,\n",
        "      warm_start, n_gpus, rank, group_name, hparams, log_directory2,\n",
        "      save_interval, backup_interval)"
      ],
      "metadata": {
        "id": "90ADKint_Sl6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "3d1abb825f11481fb47f21f86532b87f",
            "66d79bdfb26a4a95b4048a001a0e5d8f",
            "428aa3308ac64037ad2f394660be04e8",
            "9d6586b305344c4f9cd0acaefde665b0",
            "f259f394ad3d4a14b6efb6ebb8c04d0a",
            "e6f971d456574c90afa178a15056f18a",
            "6acf2a2415034af88137c0779bcd79cd",
            "fb7197802ca444ed9205f3bf9d0d531f",
            "dfa8fc377c5c4aa9ba2f4f0c8c34fe32",
            "33e7537ea8994ddc87a2ed5f8722dcce",
            "41a51bbc335e4e9fb80b304d3d32e62e"
          ]
        },
        "outputId": "0669c66f-36b2-4202-ae08-f1910ca14f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP16 Run: False\n",
            "Dynamic Loss Scaling: True\n",
            "Distributed Run: False\n",
            "cuDNN Enabled: True\n",
            "cuDNN Benchmark: False\n",
            "Loading checkpoint '/content/drive/MyDrive/colab/outdir/Alex'\n",
            "Loaded checkpoint '/content/drive/MyDrive/colab/outdir/Alex' from iteration 3000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d1abb825f11481fb47f21f86532b87f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## <font color=\"pink\"> **2. Install Tacotron2 (w/ARPAbet).** ⚙️\n",
        "\n",
        "import os\n",
        "\n",
        "print(\"Checking if TTS-TT2 repository exists...\")\n",
        "if not os.path.exists('TTS-TT2'):\n",
        "    print(\"Cloning TTS-TT2 repository...\")\n",
        "    !git clone https://github.com/NVIDIA/tacotron2.git TTS-TT2\n",
        "    print(\"TTS-TT2 repository cloned.\")\n",
        "else:\n",
        "    print(\"TTS-TT2 repository already exists.\")\n",
        "\n",
        "# --- NEW ADDITION FOR WAVEGLOW SOURCE CODE ---\n",
        "print(\"\\nChecking if WaveGlow repository exists...\")\n",
        "if not os.path.exists('TTS-TT2/waveglow'):\n",
        "    print(\"Cloning WaveGlow repository into TTS-TT2/...\")\n",
        "    !git clone https://github.com/NVIDIA/waveglow.git TTS-TT2/waveglow\n",
        "    print(\"WaveGlow repository cloned.\")\n",
        "else:\n",
        "    print(\"WaveGlow repository already exists in TTS-TT2/.\")\n",
        "# --- END NEW ADDITION ---\n",
        "\n",
        "%cd TTS-TT2\n",
        "\n",
        "print(\"\\nInstalling TTS-TT2 requirements...\")\n",
        "# Install requirements directly from the TTS-TT2 project folder\n",
        "!pip install -r requirements.txt\n",
        "!pip install phonemizer unidecode\n",
        "\n",
        "print(\"\\nApplying ARPAbet patch...\")\n",
        "# Apply ARPAbet patch (if you use it, recommended for English)\n",
        "# This usually modifies text/__init__.py and symbols.py\n",
        "!patch -p1 < Tacotron2-for-Colab-and-Kaggle/arpabet.patch\n",
        "# Note: The original patch path might be different depending on your setup.\n",
        "# If you copied the patch directly to /content/TTS-TT2, it might be just `!patch -p1 < arpabet.patch`\n",
        "# Or if it's in a specific subfolder of your copied content, adjust the path.\n",
        "# For example, if you copied `Tacotron2-for-Colab-and-Kaggle` to `/content/TTS-TT2/Tacotron2-for-Colab-and-Kaggle/`\n",
        "# then the path is correct as shown. If not, try just `!patch -p1 < arpabet.patch`\n",
        "# assuming you downloaded it to /content/TTS-TT2 directly.\n",
        "\n",
        "\n",
        "print(\"\\n--- Installation Complete for TTS-TT2 and WaveGlow Source ---\")\n",
        "print(\"Current working directory:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcJWCzxkdnWg",
        "outputId": "575cfa09-6b98-46c8-e218-17ad3d968402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking if TTS-TT2 repository exists...\n",
            "Cloning TTS-TT2 repository...\n",
            "Cloning into 'TTS-TT2'...\n",
            "remote: Enumerating objects: 412, done.\u001b[K\n",
            "remote: Counting objects: 100% (172/172), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 412 (delta 140), reused 126 (delta 126), pack-reused 240 (from 1)\u001b[K\n",
            "Receiving objects: 100% (412/412), 2.68 MiB | 6.66 MiB/s, done.\n",
            "Resolving deltas: 100% (216/216), done.\n",
            "TTS-TT2 repository cloned.\n",
            "\n",
            "Checking if WaveGlow repository exists...\n",
            "WaveGlow repository already exists in TTS-TT2/.\n",
            "/content/TTS-TT2/TTS-TT2\n",
            "\n",
            "Installing TTS-TT2 requirements...\n",
            "Collecting matplotlib==2.1.0 (from -r requirements.txt (line 1))\n",
            "  Downloading matplotlib-2.1.0.tar.gz (35.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.15.2 (from versions: 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0, 2.19.1, 2.20.0rc0, 2.20.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==1.15.2\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting phonemizer\n",
            "  Downloading phonemizer-3.3.0-py3-none-any.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: unidecode in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from phonemizer) (1.5.1)\n",
            "Collecting segments (from phonemizer)\n",
            "  Downloading segments-2.3.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: attrs>=18.1 in /usr/local/lib/python3.11/dist-packages (from phonemizer) (25.3.0)\n",
            "Collecting dlinfo (from phonemizer)\n",
            "  Downloading dlinfo-2.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from phonemizer) (4.14.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from segments->phonemizer) (2024.11.6)\n",
            "Collecting csvw>=1.5.6 (from segments->phonemizer)\n",
            "  Downloading csvw-3.5.1-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting isodate (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (2.9.0.post0)\n",
            "Collecting rfc3986<2 (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: uritemplate>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (4.2.0)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (2.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (2.32.3)\n",
            "Collecting language-tags (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Downloading language_tags-1.2.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting rdflib (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Downloading rdflib-7.1.4-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting colorama (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (4.25.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (0.27.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->csvw>=1.5.6->segments->phonemizer) (1.17.0)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib->csvw>=1.5.6->segments->phonemizer) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (2025.8.3)\n",
            "Downloading phonemizer-3.3.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.8/103.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dlinfo-2.0.0-py3-none-any.whl (3.7 kB)\n",
            "Downloading segments-2.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading csvw-3.5.1-py2.py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Downloading language_tags-1.2.0-py3-none-any.whl (213 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.4/213.4 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rdflib-7.1.4-py3-none-any.whl (565 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.1/565.1 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rfc3986, language-tags, rdflib, isodate, dlinfo, colorama, csvw, segments, phonemizer\n",
            "Successfully installed colorama-0.4.6 csvw-3.5.1 dlinfo-2.0.0 isodate-0.7.2 language-tags-1.2.0 phonemizer-3.3.0 rdflib-7.1.4 rfc3986-1.5.0 segments-2.3.0\n",
            "\n",
            "Applying ARPAbet patch...\n",
            "/bin/bash: line 1: Tacotron2-for-Colab-and-Kaggle/arpabet.patch: No such file or directory\n",
            "\n",
            "--- Installation Complete for TTS-TT2 and WaveGlow Source ---\n",
            "Current working directory: /content/TTS-TT2/TTS-TT2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## <font color=\"pink\"> **7. Perform Text-to-Speech Inference** 🗣️\n",
        "\n",
        "#@markdown #### **Model Checkpoint Path:**\n",
        "#@markdown Enter the path to your trained Tacotron2 checkpoint file on Google Drive (e.g., \"/content/drive/My Drive/Evatalk/Output/checkpoints/tacotron2_20000.pt\")\n",
        "tacotron2_checkpoint_path = \"/content/drive/My Drive/Evatalk/colab/outdir/Alex.pt\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### **WaveGlow Vocoder Setup:**\n",
        "#@markdown WaveGlow is required to convert mel spectrograms to audio. We will download a pre-trained NVIDIA WaveGlow model.\n",
        "download_waveglow = True #@param {type:\"boolean\"}\n",
        "#@markdown <small>If you already downloaded it in a previous session or manually, you can uncheck this.</small>\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### **Inference Settings:**\n",
        "text_input = \"Hello, this is a test of your new voice model. I hope it sounds great!\" #@param {type:\"string\"}\n",
        "output_filename = \"output_speech.wav\" #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import write\n",
        "import time\n",
        "\n",
        "# Ensure we are in the TTS-TT2 directory\n",
        "%cd /content/TTS-TT2/\n",
        "\n",
        "# 1. Mount Google Drive (if not already mounted)\n",
        "from google.colab import drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted.\")\n",
        "else:\n",
        "    print(\"Google Drive already mounted.\")\n",
        "\n",
        "# Ensure essential libraries and model components are available\n",
        "try:\n",
        "    from model import Tacotron2\n",
        "    from layers import TacotronSTFT\n",
        "    from audio_processing import griffin_lim\n",
        "    from text import text_to_sequence\n",
        "    from hparams import create_hparams\n",
        "    # Needed for WaveGlow\n",
        "    from waveglow.model import WaveGlow # This assumes waveglow is in a 'waveglow' subfolder\n",
        "except ImportError as e:\n",
        "    print(f\"\\n\\031[31m\\033[1m[ERROR] Required module not found: {e}\")\n",
        "    print(\"Please ensure you have run the 'Install Tacotron2' cell (Cell 2) completely.\")\n",
        "    print(\"If 'waveglow' module is missing, ensure it's cloned/downloaded correctly.\")\n",
        "    raise\n",
        "\n",
        "# Create hparams for consistency\n",
        "hparams = create_hparams()\n",
        "hparams.sampling_rate = 22050 # Ensure sampling rate is consistent\n",
        "hparams.max_wav_value = 32768.0 # Consistent with training\n",
        "\n",
        "# 2. Load Tacotron2 Model\n",
        "print(f\"\\nLoading Tacotron2 model from: {tacotron2_checkpoint_path}\")\n",
        "if not os.path.exists(tacotron2_checkpoint_path):\n",
        "    print(f\"\\033[31m\\033[1m[ERROR] Tacotron2 checkpoint not found at: {tacotron2_checkpoint_path}\")\n",
        "    print(\"Please ensure the path is correct and your model finished training and saved a checkpoint.\")\n",
        "    raise FileNotFoundError(\"Tacotron2 checkpoint not found.\")\n",
        "\n",
        "try:\n",
        "    model = Tacotron2(hparams).cuda()\n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(tacotron2_checkpoint_path, map_location='cuda')\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    print(\"Tacotron2 model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\033[31m\\033[1m[ERROR] Failed to load Tacotron2 model: {e}\")\n",
        "    print(\"This might be due to a mismatch in hparams or a corrupted checkpoint.\")\n",
        "    raise\n",
        "\n",
        "# 3. Load WaveGlow Vocoder\n",
        "waveglow_path = '/content/TTS-TT2/waveglow_256channels_universal_v5.pt' # Default path\n",
        "\n",
        "if download_waveglow and not os.path.exists(waveglow_path):\n",
        "    print(\"\\nDownloading pre-trained WaveGlow model (this may take a few minutes)...\")\n",
        "    try:\n",
        "        # Using wget to download directly\n",
        "        !wget -nc https://api.ngc.nvidia.com/v2/models/nvidia/waveglowpyt_fp32/versions/original_pyt/files/waveglow_256channels_universal_v5.pt -P /content/TTS-TT2/\n",
        "        # Move it to the expected path if downloaded to a different spot\n",
        "        if os.path.exists('/content/TTS-TT2/waveglow_256channels_universal_v5.pt'):\n",
        "            print(\"WaveGlow downloaded.\")\n",
        "        else:\n",
        "            print(\"\\033[33m\\033[1m[WARNING] WaveGlow download command ran, but file not found. Check permissions or network.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\033[31m\\033[1m[ERROR] Failed to download WaveGlow: {e}\")\n",
        "        print(\"Please try downloading manually or ensure network connectivity.\")\n",
        "        download_waveglow = False # Prevent further attempts if failed\n",
        "\n",
        "if not os.path.exists(waveglow_path):\n",
        "    if not download_waveglow: # If user unchecked and file doesn't exist\n",
        "        print(f\"\\033[31m\\033[1m[ERROR] WaveGlow model not found at: {waveglow_path}\")\n",
        "        print(\"Please set `download_waveglow = True` or provide the correct path to your WaveGlow model.\")\n",
        "        raise FileNotFoundError(\"WaveGlow model not found.\")\n",
        "    else: # If download was attempted but failed/file still not there\n",
        "        print(f\"\\033[31m\\033[1m[ERROR] WaveGlow model still not found at: {waveglow_path} after download attempt.\")\n",
        "        raise FileNotFoundError(\"WaveGlow model download failed or file missing.\")\n",
        "\n",
        "print(f\"Loading WaveGlow model from: {waveglow_path}\")\n",
        "try:\n",
        "    waveglow = torch.load(waveglow_path)['model']\n",
        "    waveglow.cuda().eval()\n",
        "    # For inference, remove the bias layer if it exists (for faster inference without artifacts)\n",
        "    for k in waveglow.convinv:\n",
        "        k.float()\n",
        "    print(\"WaveGlow model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\033[31m\\033[1m[ERROR] Failed to load WaveGlow model: {e}\")\n",
        "    print(\"This might be due to a corrupted file or compatibility issues.\")\n",
        "    raise\n",
        "\n",
        "# 4. Define Inference Function\n",
        "def synthesize_speech(text, model, waveglow, hparams, output_path=\"output.wav\"):\n",
        "    sequence = np.array(text_to_sequence(text, hparams.text_cleaners))\n",
        "    sequence = torch.autograd.Variable(\n",
        "        torch.from_numpy(sequence)).cuda().unsqueeze(0)\n",
        "\n",
        "    # Tacotron2 inference\n",
        "    print(f\"\\nSynthesizing mel-spectrogram for: \\\"{text}\\\"\")\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        mel_outputs, mel_outputs_postnet, _, alignments = model.inference(sequence)\n",
        "    tacotron2_time = time.time() - start_time\n",
        "    print(f\"Mel spectrogram generated in {tacotron2_time:.2f} seconds.\")\n",
        "\n",
        "    # WaveGlow inference\n",
        "    print(\"Generating audio with WaveGlow...\")\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        audio = waveglow.infer(mel_outputs_postnet, sigma=0.666) # sigma can be adjusted\n",
        "    waveglow_time = time.time() - start_time\n",
        "    print(f\"Audio generated in {waveglow_time:.2f} seconds.\")\n",
        "\n",
        "    audio = audio.cpu().numpy()\n",
        "    audio = audio * hparams.max_wav_value\n",
        "    audio = audio.astype('int16')\n",
        "\n",
        "    # Save audio\n",
        "    write(output_path, hparams.sampling_rate, audio)\n",
        "    print(f\"Generated audio saved to: {output_path}\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "# 5. Perform Inference\n",
        "print(\"\\n--- Starting TTS Inference ---\")\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using GPU for inference.\")\n",
        "else:\n",
        "    print(\"\\033[33m\\033[1m[WARNING] GPU not available. Inference will be very slow on CPU.\")\n",
        "\n",
        "try:\n",
        "    synthesize_speech(text_input, model, waveglow, hparams, output_filename)\n",
        "except Exception as e:\n",
        "    print(f\"\\033[31m\\033[1m[ERROR] An error occurred during speech synthesis: {e}\")\n",
        "    print(\"Please check your input text, model, and vocoder setup.\")\n",
        "\n",
        "print(\"\\n\\033[32m\\033[1mTTS Inference complete!\")\n",
        "print(\"You can download the generated WAV file from the Colab file browser (look for 'output_speech.wav').\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "SNSNTcIZbd3y",
        "outputId": "51fe945d-9024-4f68-e0b1-31db12f1941d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TTS-TT2\n",
            "Google Drive already mounted.\n",
            "\n",
            "\u0019[31m\u001b[1m[ERROR] Required module not found: No module named 'waveglow.model'\n",
            "Please ensure you have run the 'Install Tacotron2' cell (Cell 2) completely.\n",
            "If 'waveglow' module is missing, ensure it's cloned/downloaded correctly.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'waveglow.model'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-27344522.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mhparams\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_hparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Needed for WaveGlow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mwaveglow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWaveGlow\u001b[0m \u001b[0;31m# This assumes waveglow is in a 'waveglow' subfolder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n\\031[31m\\033[1m[ERROR] Required module not found: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'waveglow.model'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final notes*\\\n",
        "\n",
        "## A good training looks like this:\n",
        "![img.png](https://media.discordapp.net/attachments/835971020569051216/851469553355587614/download_2.png)\n"
      ],
      "metadata": {
        "id": "6dAP4f7H_pHy"
      }
    }
  ]
}